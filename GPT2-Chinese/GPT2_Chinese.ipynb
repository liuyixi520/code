{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K7QADQvokUzH",
        "outputId": "2bc04ec6-ae6f-44e0-933e-0786fa13e86a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              " function ClickConnect(){\n",
              "   btn = document.querySelector(\"colab-connect-button\")\n",
              "   if (btn != null){\n",
              "     console.log(\"Click colab-connect-button\"); \n",
              "     btn.click() \n",
              "     }\n",
              "   \n",
              "   btn = document.getElementById('ok')\n",
              "   if (btn != null){\n",
              "     console.log(\"Click reconnect\"); \n",
              "     btn.click() \n",
              "     }\n",
              "  }\n",
              "  \n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# 用来保持连接的\n",
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        " function ClickConnect(){\n",
        "   btn = document.querySelector(\"colab-connect-button\")\n",
        "   if (btn != null){\n",
        "     console.log(\"Click colab-connect-button\"); \n",
        "     btn.click() \n",
        "     }\n",
        "   \n",
        "   btn = document.getElementById('ok')\n",
        "   if (btn != null){\n",
        "     console.log(\"Click reconnect\"); \n",
        "     btn.click() \n",
        "     }\n",
        "  }\n",
        "  \n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w58kD9vbAARZ"
      },
      "source": [
        "# 1. 安装环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inPo4CMrAGMU",
        "outputId": "5fa08aa4-0b70-4657-89e6-c0a4d82f85db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GPT2-Chinese'...\n",
            "remote: Enumerating objects: 280, done.\u001b[K\n",
            "remote: Total 280 (delta 0), reused 0 (delta 0), pack-reused 280\u001b[K\n",
            "Receiving objects: 100% (280/280), 13.44 MiB | 6.50 MiB/s, done.\n",
            "Resolving deltas: 100% (140/140), done.\n"
          ]
        }
      ],
      "source": [
        "# 克隆的仓库在./GPT2-Chinese 目录下\n",
        "!git clone https://github.com/Morizeyao/GPT2-Chinese.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx3q7ez0VGY5",
        "outputId": "538d0393-081e-49a3-87bf-26ee63a8247b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==2.1.1\n",
            "  Downloading transformers-2.1.1-py3-none-any.whl (311 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 36.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 39.5 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 44.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 163 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 225 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 245 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 256 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 266 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 276 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 296 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 311 kB 29.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 2)) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 4)) (4.64.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 5)) (0.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 6)) (2.8.0)\n",
            "Collecting tb-nightly\n",
            "  Downloading tb_nightly-2.10.0a20220520-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from -r ./GPT2-Chinese/requirements.txt (line 8)) (0.16.0)\n",
            "Collecting thulac\n",
            "  Downloading thulac-0.2.1.tar.gz (52.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 52.9 MB 46 kB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 68.2 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.23.5-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r ./GPT2-Chinese/requirements.txt (line 2)) (4.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r ./GPT2-Chinese/requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (3.3.7)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.46.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->-r ./GPT2-Chinese/requirements.txt (line 7)) (3.2.0)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.4 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.27.0,>=1.26.5\n",
            "  Downloading botocore-1.26.5-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 57.7 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 80.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.5->boto3->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.1.1->-r ./GPT2-Chinese/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r ./GPT2-Chinese/requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r ./GPT2-Chinese/requirements.txt (line 5)) (1.4.1)\n",
            "Building wheels for collected packages: thulac, sacremoses\n",
            "  Building wheel for thulac (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thulac: filename=thulac-0.2.1-py3-none-any.whl size=53141671 sha256=6aba296513449f49eaaf37fb82dd041c1e72e1760c924eed0bd8e5b9b9f1901b\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/37/f3/be4ae10faf0fbf35cc192469b737ead6f8f99404bcd82fb2e0\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=379c5c14ecfb0e713544e7e5c8473e51a2b5e0b29b8ab8ce6a378963bcb98b85\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built thulac sacremoses\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, transformers, thulac, tb-nightly\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.23.5 botocore-1.26.5 jmespath-1.0.0 s3transfer-0.5.2 sacremoses-0.0.53 sentencepiece-0.1.96 tb-nightly-2.10.0a20220520 thulac-0.2.1 transformers-2.1.1 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "# 安装需求文件\n",
        "!python3 -m pip install -r ./GPT2-Chinese/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_uVk6TyGXGe",
        "outputId": "1ab2be81-e9e3-419b-aa99-abc49fc5d206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT2-Chinese\n"
          ]
        }
      ],
      "source": [
        "# 移动到GPT2-Chinese 路径\n",
        "%cd ./GPT2-Chinese"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEYIRUGxCZpM",
        "outputId": "c4e391c6-12b8-4cc1-a9fc-805358e5a7f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT2-Chinese/pretrained\n"
          ]
        }
      ],
      "source": [
        "!mkdir pretrained\n",
        "%cd ./pretrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2bJSuu_HvXg"
      },
      "source": [
        "**下载pretrained model 到新创建的pretrained文件夹**\n",
        "\n",
        "用的是通用中文模型（小）\n",
        "\n",
        "想要用别的模型 到这个repo 找gdrive 链接 https://github.com/Morizeyao/GPT2-Chinese\n",
        "\n",
        "然后拷贝到自己的gdrive就可以，右上三个点-> add shortcut ->然后在快捷方式里对应的文件选make copy\n",
        "\n",
        "再将copy下来的文件改为谁都有修改权限，将分享链接/d/后面的id复制下来就可以下载了"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl0gWAfpApOB",
        "outputId": "a7cb202a-fc0c-46d6-e508-d21150f25c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16FmH5ZjpejKLNUtmdTfteZlREQkuVyl5\n",
            "To: /content/GPT2-Chinese/pretrained/pytorch_model.bin\n",
            "100% 421M/421M [00:04<00:00, 92.8MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13Trje1N7YU_1U9iGLpMjRrIFZlg5HC1-\n",
            "To: /content/GPT2-Chinese/pretrained/config.json\n",
            "100% 605/605 [00:00<00:00, 1.21MB/s]\n",
            "config.json  pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 16FmH5ZjpejKLNUtmdTfteZlREQkuVyl5\n",
        "!gdown --id 13Trje1N7YU_1U9iGLpMjRrIFZlg5HC1-\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkBLFhPjHAgG",
        "outputId": "0d646675-edf9-4f76-fdf7-c5cf733f897f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT2-Chinese\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlujTb5ZHKty"
      },
      "source": [
        "# 2. 转换训练用文本\n",
        "先将你的训练txt发到colab file，以train.txt命名, 记得放在GPT2-Chinese文件夹下"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BtSxr4_VIBTp"
      },
      "outputs": [],
      "source": [
        "# 删掉原本的json\n",
        "!rm -f train.json\n",
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_kVtKWjaIh_5"
      },
      "outputs": [],
      "source": [
        "# 根据你的训练文本创建train.json\n",
        "\n",
        "\"\"\"\n",
        "Author: godweiyang\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "\n",
        "dic = {}\n",
        "with open(\"train.txt\", \"r\", encoding=\"utf8\") as f:\n",
        "    merge_line = \"\"\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        merge_line += line\n",
        "        if len(merge_line) > 500:\n",
        "            dic[merge_line] = 1\n",
        "            merge_line = \"\"\n",
        "\n",
        "with open(\"./data/train.json\", \"w\", encoding=\"utf8\") as f:\n",
        "    json.dump(dic, f, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NXbDOrrJBjY"
      },
      "source": [
        "# 3. 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "Lb_LDuK4JPGK",
        "outputId": "e7be41ac-162d-4699-c809-fd57f74d9e69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-20 23:47:28.062920: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "usage: train.py [-h] [--device DEVICE] [--model_config MODEL_CONFIG]\n",
            "                [--tokenizer_path TOKENIZER_PATH]\n",
            "                [--raw_data_path RAW_DATA_PATH]\n",
            "                [--tokenized_data_path TOKENIZED_DATA_PATH] [--raw]\n",
            "                [--epochs EPOCHS] [--batch_size BATCH_SIZE] [--lr LR]\n",
            "                [--warmup_steps WARMUP_STEPS] [--log_step LOG_STEP]\n",
            "                [--stride STRIDE]\n",
            "                [--gradient_accumulation GRADIENT_ACCUMULATION] [--fp16]\n",
            "                [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                [--max_grad_norm MAX_GRAD_NORM] [--num_pieces NUM_PIECES]\n",
            "                [--min_length MIN_LENGTH] [--output_dir OUTPUT_DIR]\n",
            "                [--pretrained_model PRETRAINED_MODEL]\n",
            "                [--writer_dir WRITER_DIR] [--segment] [--bpe_token]\n",
            "                [--encoder_json ENCODER_JSON] [--vocab_bpe VOCAB_BPE]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --device DEVICE       设置使用哪些显卡\n",
            "  --model_config MODEL_CONFIG\n",
            "                        选择模型参数\n",
            "  --tokenizer_path TOKENIZER_PATH\n",
            "                        选择词库\n",
            "  --raw_data_path RAW_DATA_PATH\n",
            "                        原始训练语料\n",
            "  --tokenized_data_path TOKENIZED_DATA_PATH\n",
            "                        tokenized语料存放位置\n",
            "  --raw                 是否先做tokenize\n",
            "  --epochs EPOCHS       训练循环\n",
            "  --batch_size BATCH_SIZE\n",
            "                        训练batch size\n",
            "  --lr LR               学习率\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        warm up步数\n",
            "  --log_step LOG_STEP   多少步汇报一次loss，设置为gradient accumulation的整数倍\n",
            "  --stride STRIDE       训练时取训练数据的窗口步长\n",
            "  --gradient_accumulation GRADIENT_ACCUMULATION\n",
            "                        梯度积累\n",
            "  --fp16                混合精度\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "  --num_pieces NUM_PIECES\n",
            "                        将训练语料分成多少份\n",
            "  --min_length MIN_LENGTH\n",
            "                        最短收录文章长度\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        模型输出路径\n",
            "  --pretrained_model PRETRAINED_MODEL\n",
            "                        模型训练起点路径\n",
            "  --writer_dir WRITER_DIR\n",
            "                        Tensorboard路径\n",
            "  --segment             中文以词为单位\n",
            "  --bpe_token           subword\n",
            "  --encoder_json ENCODER_JSON\n",
            "                        encoder.json\n",
            "  --vocab_bpe VOCAB_BPE\n",
            "                        vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "# 参数用途\n",
        "!python train.py -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9z9-HKiUwmL",
        "outputId": "5c6bf6e8-3f8d-48db-8621-6eac8f1af05c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-20 23:47:33.688990: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "args:\n",
            "Namespace(batch_size=2, bpe_token=False, device='0,1,2,3', encoder_json='tokenizations/encoder.json', epochs=1, fp16=False, fp16_opt_level='O1', gradient_accumulation=1, log_step=1, lr=0.00015, max_grad_norm=1.0, min_length=3, model_config='config/model_config_small.json', num_pieces=1, output_dir='model/', pretrained_model='pretrained', raw=True, raw_data_path='data/train.json', segment=False, stride=768, tokenized_data_path='data/tokenized/', tokenizer_path='cache/vocab_small.txt', vocab_bpe='tokenizations/vocab.bpe', warmup_steps=2000, writer_dir='tensorboard_summary/')\n",
            "config:\n",
            "{\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 10,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_labels\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 13317\n",
            "}\n",
            "\n",
            "using device: cuda\n",
            "building files\n",
            "reading lines\n"
          ]
        }
      ],
      "source": [
        "# 超过memory就把batch_size改小点\n",
        "!python train.py --raw --epochs 1 --batch_size 2 --num_pieces 1 --min_length 3 --pretrained_model pretrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfsAF8j1J4xw"
      },
      "source": [
        "# 4. 生成文本\n",
        "一定要等训练完不然会报错\n",
        "\n",
        "原repo不能断点续传\n",
        "\n",
        "如果想再已经训练的模型基础上继续训练可以用`./model`下面存储的model替换pretrained里的model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kekMcNGMc9n0"
      },
      "outputs": [],
      "source": [
        "# length决定长度， nsamples 后面决定生成多少个例子。\n",
        "input = \"半年时间，眨眼便过。\"\n",
        "!python3 generate.py --length=500 --nsamples=3 --prefix=$input --fast_pattern"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VwUBhQeG2tW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GPT2-Chinese.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}