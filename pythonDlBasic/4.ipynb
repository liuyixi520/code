{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络的学习\n",
    "- 从训练数据集中获得最优参数的过程，记神经网络的反向传播过程\n",
    "- 深度学习是一种新的编程方法，摆脱了以往以人为中心的思想，所有的东西都让数据发声\n",
    "## 传统算法&机器学习&深度学习\n",
    "- 传统算法：人能想到的算法,然后编写算法规则，即把假设空间写死\n",
    "- 机器学习：人能想到的特征，提取特征向量，机器学习（SVM/KNN）\n",
    "- 深度学习：数据->答案\n",
    "- 所以也称机器学习是一种端到端的学习方式\n",
    "## 以识别手写数字为例说明三种算法的思路\n",
    "### 传统算法\n",
    "- 把(28*28)**256所有的情况都枚举出来\n",
    "- 然后写分支判断语句，判断每种情况是哪种分类\n",
    "- 但这种情况是不行的，因为(28*28)**256是一个天文数字，写不出来这样的代码\n",
    "### 机器学习\n",
    "- 提取特征：比如说边缘、角点、纹理、统计特征等\n",
    "- 然后使用机器学习算法，比如SVM/KNN\n",
    "- 最后就能比较好的将图片分类\n",
    "### 深度学习\n",
    "- 将图片数据直接喂给深度学习算法，比如神经网络\n",
    "- 通过反向传播机制，让算法自己找到最好的假设空间\n",
    "- 最后能比较好的进行图片分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 均方误差\n",
    "- 使用公式表达均方误差：$ E = 1/2 \\sum_k (yk - tk)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "# 使用softmax预测出来的一组数字的概率\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "# 使用one-hot编码的标签，这个样本的真实标签是2\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# 计算示例中的均方误差\n",
    "# 也就说这个样本所产生的均方误差大概是0.1\n",
    "print(mean_squared_error(np.array(y), np.array(t)))\n",
    "\n",
    "# 再来计算一个样本的均方误差\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "# 假设真实标签还是2\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# 由于预测出来是7的概率最大，所以这个样本的损失还是比较大的\n",
    "print(mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵误差\n",
    "- 使用公式表达交叉熵误差：$ E = - \\sum_k tk \\log(yk) $\n",
    "- 相对于均方误差，这个误差只考虑真实标签那个预测情况，预测概率越接近于1，误差越小，反之越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "# 以上面mnist的结果，计算一下交叉熵的损失\n",
    "def cross_entropy_error(y, t):\n",
    "    # 为了防止log(0)对结果溢出，加上一个比较小的量，根据softmax的例子\n",
    "    # 即使加了这delta也不影响最终的概率分布\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "\n",
    "# 预测比较失败的那个交叉熵损失是多少\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mini-batch学习\n",
    "- 机器学习的过程就是对损失函数学习的过程\n",
    "- 前面介绍的损失只是对单个样本的损失，损失函数的求解是对所有样本的损失的求和\n",
    "- 但是所有的样本又太大了，学习起来成本太高\n",
    "- 所以训练的时候常常将一些样本捆绑起来，形成一个batch，然后对一个batch进行学习\n",
    "- 一个batch的交叉熵损失函数可以用公式表达为：$ E = -1/N \\sum_N \\sum_k tk \\log(yk) $\n",
    "- 其中N是batch的大小，k是每个样本的索引，yk是预测的概率，tk是真实的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n",
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 使用keras加载mnist数据集\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_train.shape)\n",
    "print(t_test.shape)\n",
    "# 把特征空间拉平，并转换成[0, 1]区间的值，把28*28的图片变成一维的\n",
    "# 把标签空间转换成one-hot编码\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "t_train = np.eye(10)[t_train.astype('int32')]\n",
    "t_test = np.eye(10)[t_test.astype('int32')]\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_train.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17226 16465 58713  9539 16037 39596 51073 36135 21423  3347]\n",
      "(10, 784)\n",
      "(10, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 随机从训练集中抽取一些数据，用于训练\n",
    "import numpy as np\n",
    "# 从[0, 60000)区间随机抽取10个数字\n",
    "batch_mask = np.random.choice(x_train.shape[0], 10)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "print(batch_mask)\n",
    "print(x_batch.shape)\n",
    "print(t_batch.shape)\n",
    "print(type(x_batch))\n",
    "print(type(t_batch))\n",
    "print(x_batch)\n",
    "print(t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个batch的交叉熵损失函数定义\n",
    "def cross_entropy_error(y, t):\n",
    "    # 如果仅有一个样本，那么就给它增加一个维度，表示是单一样本\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.302584092994546\n",
      "(100,)\n",
      "91.00478626443056\n"
     ]
    }
   ],
   "source": [
    "# 单个样本的交叉熵损失\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "# 多个样本的交叉熵损失\n",
    "y = np.array(y*10)\n",
    "print(y.shape)\n",
    "print(cross_entropy_error(y, t_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1021d31412234ebea20baf61e8fdbc8c22153a59dde70b52bcafccce89619e30"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
