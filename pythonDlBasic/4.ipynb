{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络的学习\n",
    "- 从训练数据集中获得最优参数的过程，记神经网络的反向传播过程\n",
    "- 深度学习是一种新的编程方法，摆脱了以往以人为中心的思想，所有的东西都让数据发声\n",
    "## 传统算法&机器学习&深度学习\n",
    "- 传统算法：人能想到的算法,然后编写算法规则，即把假设空间写死\n",
    "- 机器学习：人能想到的特征，提取特征向量，机器学习（SVM/KNN）\n",
    "- 深度学习：数据->答案\n",
    "- 所以也称机器学习是一种端到端的学习方式\n",
    "## 以识别手写数字为例说明三种算法的思路\n",
    "### 传统算法\n",
    "- 把(28*28)**256所有的情况都枚举出来\n",
    "- 然后写分支判断语句，判断每种情况是哪种分类\n",
    "- 但这种情况是不行的，因为(28*28)**256是一个天文数字，写不出来这样的代码\n",
    "### 机器学习\n",
    "- 提取特征：比如说边缘、角点、纹理、统计特征等\n",
    "- 然后使用机器学习算法，比如SVM/KNN\n",
    "- 最后就能比较好的将图片分类\n",
    "### 深度学习\n",
    "- 将图片数据直接喂给深度学习算法，比如神经网络\n",
    "- 通过反向传播机制，让算法自己找到最好的假设空间\n",
    "- 最后能比较好的进行图片分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 均方误差\n",
    "- 使用公式表达均方误差：$ E = 1/2 \\sum_k (yk - tk)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "# 使用softmax预测出来的一组数字的概率\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "# 使用one-hot编码的标签，这个样本的真实标签是2\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# 计算示例中的均方误差\n",
    "# 也就说这个样本所产生的均方误差大概是0.1\n",
    "print(mean_squared_error(np.array(y), np.array(t)))\n",
    "\n",
    "# 再来计算一个样本的均方误差\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "# 假设真实标签还是2\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# 由于预测出来是7的概率最大，所以这个样本的损失还是比较大的\n",
    "print(mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵误差\n",
    "- 使用公式表达交叉熵误差：$ E = - \\sum_k tk \\log(yk) $\n",
    "- 相对于均方误差，这个误差只考虑真实标签那个预测情况，预测概率越接近于1，误差越小，反之越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "# 以上面mnist的结果，计算一下交叉熵的损失\n",
    "def cross_entropy_error(y, t):\n",
    "    # 为了防止log(0)对结果溢出，加上一个比较小的量，根据softmax的例子\n",
    "    # 即使加了这delta也不影响最终的概率分布\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "\n",
    "# 预测比较失败的那个交叉熵损失是多少\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1021d31412234ebea20baf61e8fdbc8c22153a59dde70b52bcafccce89619e30"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
