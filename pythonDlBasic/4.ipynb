{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络的学习\n",
    "- 从训练数据集中获得最优参数的过程，记神经网络的反向传播过程\n",
    "- 深度学习是一种新的编程方法，摆脱了以往以人为中心的思想，所有的东西都让数据发声\n",
    "## 传统算法&机器学习&深度学习\n",
    "- 传统算法：人能想到的算法,然后编写算法规则，即把假设空间写死\n",
    "- 机器学习：人能想到的特征，提取特征向量，机器学习（SVM/KNN）\n",
    "- 深度学习：数据->答案\n",
    "- 所以也称机器学习是一种端到端的学习方式\n",
    "## 以识别手写数字为例说明三种算法的思路\n",
    "### 传统算法\n",
    "- 把(28*28)**256所有的情况都枚举出来\n",
    "- 然后写分支判断语句，判断每种情况是哪种分类\n",
    "- 但这种情况是不行的，因为(28*28)**256是一个天文数字，写不出来这样的代码\n",
    "### 机器学习\n",
    "- 提取特征：比如说边缘、角点、纹理、统计特征等\n",
    "- 然后使用机器学习算法，比如SVM/KNN\n",
    "- 最后就能比较好的将图片分类\n",
    "### 深度学习\n",
    "- 将图片数据直接喂给深度学习算法，比如神经网络\n",
    "- 通过反向传播机制，让算法自己找到最好的假设空间\n",
    "- 最后能比较好的进行图片分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 均方误差\n",
    "- 使用公式表达均方误差：$ E = 1/2 \\sum_k (yk - tk)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "# 使用softmax预测出来的一组数字的概率\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "# 使用one-hot编码的标签，这个样本的真实标签是2\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# 计算示例中的均方误差\n",
    "# 也就说这个样本所产生的均方误差大概是0.1\n",
    "print(mean_squared_error(np.array(y), np.array(t)))\n",
    "\n",
    "# 再来计算一个样本的均方误差\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "# 假设真实标签还是2\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# 由于预测出来是7的概率最大，所以这个样本的损失还是比较大的\n",
    "print(mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵误差\n",
    "- 使用公式表达交叉熵误差：$ E = - \\sum_k tk \\log(yk) $\n",
    "- 相对于均方误差，这个误差只考虑真实标签那个预测情况，预测概率越接近于1，误差越小，反之越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.3025840929945454\n"
     ]
    }
   ],
   "source": [
    "# 以上面mnist的结果，计算一下交叉熵的损失\n",
    "def cross_entropy_error(y, t):\n",
    "    # 为了防止log(0)对结果溢出，加上一个比较小的量，根据softmax的例子\n",
    "    # 即使加了这delta也不影响最终的概率分布\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "\n",
    "# 预测比较失败的那个交叉熵损失是多少\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mini-batch学习\n",
    "- 机器学习的过程就是对损失函数学习的过程\n",
    "- 前面介绍的损失只是对单个样本的损失，损失函数的求解是对所有样本的损失的求和\n",
    "- 但是所有的样本又太大了，学习起来成本太高\n",
    "- 所以训练的时候常常将一些样本捆绑起来，形成一个batch，然后对一个batch进行学习\n",
    "- 一个batch的交叉熵损失函数可以用公式表达为：$ E = -1/N \\sum_N \\sum_k tk \\log(yk) $\n",
    "- 其中N是batch的大小，k是每个样本的索引，yk是预测的概率，tk是真实的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 17:18:47.449141: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-25 17:18:47.449176: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n",
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 使用keras加载mnist数据集\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_train.shape)\n",
    "print(t_test.shape)\n",
    "# 把特征空间拉平，并转换成[0, 1]区间的值，把28*28的图片变成一维的\n",
    "# 把标签空间转换成one-hot编码\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "t_train = np.eye(10)[t_train.astype('int32')]\n",
    "t_test = np.eye(10)[t_test.astype('int32')]\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_train.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16950 39420  8350 13679 51388 11157 25590  1644 50493 32269]\n",
      "(10, 784)\n",
      "(10, 10)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 随机从训练集中抽取一些数据，用于训练\n",
    "import numpy as np\n",
    "# 从[0, 60000)区间随机抽取10个数字\n",
    "batch_mask = np.random.choice(x_train.shape[0], 10)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "print(batch_mask)\n",
    "print(x_batch.shape)\n",
    "print(t_batch.shape)\n",
    "print(type(x_batch))\n",
    "print(type(t_batch))\n",
    "print(x_batch)\n",
    "print(t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个batch的交叉熵损失函数定义\n",
    "def cross_entropy_error(y, t):\n",
    "    # 如果仅有一个样本，那么就给它增加一个维度，表示是单一样本\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3025840929945454\n",
      "(100,)\n",
      "94.18283726144865\n"
     ]
    }
   ],
   "source": [
    "# 单个样本的交叉熵损失\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "# 多个样本的交叉熵损失\n",
    "# 模拟生成10个样本的概率分布\n",
    "y = np.array(y*10)\n",
    "print(y.shape)\n",
    "print(cross_entropy_error(y, t_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么必须要有损失函数\n",
    "- 问题：深度学习模型最终的目的是为了提高准确率，模型的学习策略可以设置为提高学习率就可以了！但是为什么还要有损失函数的出现？\n",
    "- 如果设置了损失函数作为学习率，会导致整个学习的过程停滞不前——参数的导数大部分都会变成0\n",
    "- 在深度学习得到参数时，如果这个参数函数的导数为正，那么我们就减小这个参数；反之如果导数为负，那么我们就增大这个参数，最后慢慢的变化，就会使得最后的模型变好\n",
    "- 但是精度函数是一个离散的函数，比如说批量是100时，这个精度只会在[0,1,0.01]这100个值选择，比如说我们一组参数使得精度为32.1%，另一组参数为32.2%。那么模型是感知不到参数的调整会让模型变优的。\n",
    "- 反之损失函数是一个连续的函数就不存在这个问题\n",
    "- 同样的，一般不能使用阶跃函数作为激活函数也是这个道理，因为阶跃函数是一个离散的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数值微分\n",
    "### 导数\n",
    "导数的定义可以用公式表达为：$$ \\frac{\\partial f(x)}{\\partial x} = \\lim_{\\epsilon \\to 0} \\frac{f(x + \\epsilon) - f(x)}{\\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不好的实现\n",
    "# 注意这里函数式编程的技巧，输入的f是一个函数\n",
    "import numpy as np\n",
    "def numerical_diff_bad(f, x):\n",
    "    delta = np.float32(1e-50)\n",
    "    return (f(x+delta)-f(x))/delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "True\n",
      "1e-04\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 上面的函数有两个问题\n",
    "# 一个问题问题是：delta的值太小，会被当成0\n",
    "import numpy as np\n",
    "print(np.float32(1e-50))\n",
    "print(np.float32(1e-50)==0)\n",
    "print(np.float32(1e-4))\n",
    "print(np.float32(1e-4)==0)\n",
    "# 另一个问题是：我们想要的是在x点处的导数，这样求出来是[x, x+delta]之间的导数\n",
    "# 解决的办法是：所以要在左右两边去一个均值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def numerical_diff(f, x):\n",
    "    delta = 1e-4\n",
    "    return (f(x+delta)-f(x-delta))/(2*delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用numerical_diff函数求导\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "# 画出来上面的函数曲线\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个函数在5这个点的导数是：0.1999999999990898\n",
      "这个函数在10这个点的导数是：0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "# 来计算在5, 10两个点的导数\n",
    "# 根据高中知识上面那个函数的导数公式：g(x)=0.02x + 0.1\n",
    "# 所以计算的结果是对的\n",
    "print(f'这个函数在5这个点的导数是：{numerical_diff(function_1, 5)}')\n",
    "print(f'这个函数在10这个点的导数是：{numerical_diff(function_1, 10)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 偏导数\n",
    "- 前面的导数只是说明了自变量只有一个的情况下的导数情况\n",
    "- 这里说明自变量是多个的情况下的导数\n",
    "- 可以简单理解为将其他变量都固定下来，只求一个变量的导数\n",
    "- 最后的结果可以这样解释\n",
    "- 原来的函数图像是一个三维的\n",
    "- 在x[0]和y组成的平面中在[3,4]这个点上的斜率是6\n",
    "- 在x[1]和y组成的平面中在[3,4]这个点上的斜率是8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个函数在[3, 4]这个点的偏导数是：6.00000000000378\n",
      "这个函数在[3, 4]这个点的偏导数是：7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 与前面不同的是，前面x是一个标量，这里x是一个向量\n",
    "def function_2(x):\n",
    "    # return x[0]**2 + x[1]**2\n",
    "    return np.sum(x**2)\n",
    "# 现在比如说要求[3, 4]这个点的偏导数\n",
    "# 重新定义一个函数，将x[1]固定在4这个点\n",
    "def function_tmp1(x):\n",
    "    return x**2 + 4**2\n",
    "# 重新定义一个函数，将x[0]固定在3这个点\n",
    "def function_tmp2(x):\n",
    "    return 3**2 + x**2\n",
    "# 求出来x[0]的偏导数\n",
    "print(f'这个函数在[3, 4]这个点的偏导数是：{numerical_diff(function_tmp1, 3)}')\n",
    "# 求出来x[1]的偏导数\n",
    "print(f'这个函数在[3, 4]这个点的偏导数是：{numerical_diff(function_tmp2, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度\n",
    "- 在某个点上的梯度是指这个点偏导数\n",
    "- 梯度的维度和输入的维度一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def numerical_gradient(f, x):\n",
    "    delta = 1e-4\n",
    "    # 梯度的维度和x的维度一样\n",
    "    grad = np.zeros_like(x)\n",
    "    # 对每个维度求偏导\n",
    "    for idx in range(x.size):\n",
    "        # 当前维度，要求解梯度的那个点的坐标\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+delta)\n",
    "        x[idx] = tmp_val + delta\n",
    "        fx1 = f(x)\n",
    "        # f(x-delta)\n",
    "        x[idx] = tmp_val - delta\n",
    "        fx2 = f(x)\n",
    "        # 求偏导\n",
    "        grad[idx] = (fx1-fx2)/(2*delta)\n",
    "        # 恢复x的值\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 测试一下函数2的梯度\n",
    "import numpy as np\n",
    "# x是一个向量\n",
    "def function_2(x):\n",
    "    # return x[0]**2 + x[1]**2\n",
    "    return np.sum(x**2)\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度法\n",
    "- 深度学习的过程是使用梯度法寻找最优参数的过程\n",
    "- 梯度法可以用公式表达为：\n",
    "$$ x_0 = x_0 - \\alpha \\frac{\\partial f}{\\partial x_0} $$\n",
    "$$ x_1 = x_1 - \\alpha \\frac{\\partial f}{\\partial x_1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用简单的函数实现梯度下降\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        # 滑向最低点的过程\n",
    "        x -= lr*grad\n",
    "    # 返回最低点的那个梯度\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试一下\n",
    "def function_2(x):\n",
    "    return np.sum(x**2)\n",
    "# 初始化小球在的那个点\n",
    "x = np.array([-3.0, 4.0])\n",
    "# 从可视化的那个图形可以看出来，最低调就是x[0], x[1]导数都是0的那个点\n",
    "gradient_descent(function_2, init_x=x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n"
     ]
    }
   ],
   "source": [
    "# 学习率过大，模型最后收敛的并不好\n",
    "x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=x, lr=10.0, step_num=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 学习率过小，模型最后收敛的也不好\n",
    "x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=x, lr=1e-10, step_num=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的梯度\n",
    "- 也是一个矩阵的梯度\n",
    "- 下面这个例子中，假设神经网络只有一层，形状是(2,3)。暂且忽略偏置项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这设计python类中的函数的时候，如果要自己调用自己，那么一定要传入self参数，否则会报错\n",
    "class simpleNet:\n",
    "    def cross_entropy_error(self, y, t):\n",
    "        return -np.sum(t*np.log(y))\n",
    "    def softmax(self, x):\n",
    "        return np.exp(x)/np.sum(np.exp(x))\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = self.softmax(z)\n",
    "        loss = self.cross_entropy_error(y, t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重参数是：[[ 0.47412058  0.49296901  1.58017092]\n",
      " [-0.83625698 -1.15110694 -1.60922946]]\n",
      "这个网络的预测结果是：[-0.46815893 -0.74021484 -0.50020396]\n",
      "预测最大的那个索引是：0\n",
      "这个网络的损失是：1.0364472338066975\n"
     ]
    }
   ],
   "source": [
    "# 验证上面的简单的神经网络\n",
    "net = simpleNet()\n",
    "print(f'权重参数是：{net.W}')\n",
    "# 给定一个假设样本的特征和标签\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "p = net.predict(x)\n",
    "print(f'这个网络的预测结果是：{p}')\n",
    "# 返回最大的索引，和实际结果差不多\n",
    "print(f'预测最大的那个索引是：{np.argmax(p)}')\n",
    "# 计算损失\n",
    "print(f'这个网络的损失是：{net.loss(x, t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与前面的不同的是，这里输入的张量是2D的，所以重新使用书中的函数\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21975811  0.16741429 -0.38717241]\n",
      " [ 0.32963717  0.25112144 -0.58075861]]\n"
     ]
    }
   ],
   "source": [
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习算法的实现\n",
    "### 神经网络的学习步骤\n",
    "1. 随机选择一个mini-batch，我们的优化目标就是让这个mini-batch的损失函数最小\n",
    "2. 计算出来各个权重参数的梯度\n",
    "3. 沿着梯度的方向，对权重参数做微小的调整，使得损失函数可以变小\n",
    "4. 回到步骤一，重复上面的三个步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 书中的functions脚本\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype=np.int)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "    \n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    grad = np.zeros_like(x)\n",
    "    grad[x>=0] = 1\n",
    "    return grad\n",
    "    \n",
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)   # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def sum_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "def softmax_loss(X, t):\n",
    "    y = softmax(X)\n",
    "    return cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 书中的gradients脚本\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "def _numerical_gradient_1d(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient_2d(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_1d(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_1d(f, x)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 随机初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        # 下面这个求梯度的函数不是这个类的函数，否则要写成self.numerical_gradient\n",
    "        # 这个是用的书里脚本gradient.py中定义的那个函数\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n",
      "[[ 0.01265912  0.00255623  0.02018787 ...  0.00040228 -0.00966899\n",
      "   0.00660045]\n",
      " [ 0.00750564  0.01046873 -0.01161288 ...  0.00203962 -0.00029753\n",
      "  -0.00074265]\n",
      " [-0.00723639  0.00241832  0.00089773 ... -0.0053189  -0.00486465\n",
      "   0.00344039]\n",
      " ...\n",
      " [-0.00785087  0.0178178  -0.01771919 ... -0.00449164 -0.01113843\n",
      "  -0.0224694 ]\n",
      " [-0.00207771 -0.00252004  0.0154653  ... -0.00474729  0.00122851\n",
      "  -0.00069408]\n",
      " [ 0.02348847 -0.01537109 -0.01916646 ...  0.01865788  0.00606719\n",
      "  -0.00054185]]\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)\n",
    "print(net.params['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10004286 0.10464642 0.10168182 0.0920164  0.10909953 0.09455619\n",
      "  0.0978269  0.09469302 0.10670536 0.09873149]\n",
      " [0.10013325 0.10436338 0.10174684 0.09251325 0.10918128 0.09431763\n",
      "  0.09781718 0.09448949 0.10717058 0.09826712]\n",
      " [0.1002524  0.10429226 0.10180613 0.09252038 0.1088027  0.09468104\n",
      "  0.09771737 0.09462513 0.10681697 0.09848562]\n",
      " [0.10008906 0.10417006 0.10138654 0.09293945 0.10928399 0.09439896\n",
      "  0.09791424 0.09440648 0.10667607 0.09873515]\n",
      " [0.10008238 0.10416021 0.10147249 0.09254177 0.10941181 0.09458838\n",
      "  0.09768941 0.09475855 0.10662736 0.09866765]\n",
      " [0.09997    0.10435197 0.10168184 0.09228505 0.10916374 0.09477858\n",
      "  0.09792177 0.09443514 0.10682328 0.09858863]\n",
      " [0.10030807 0.10440664 0.10171218 0.09239325 0.109162   0.09468447\n",
      "  0.09776815 0.09426475 0.10660308 0.09869741]\n",
      " [0.10012959 0.10423257 0.10179107 0.09223245 0.10924475 0.09458081\n",
      "  0.09805121 0.09421286 0.10679736 0.09872733]\n",
      " [0.09989354 0.10459233 0.10157199 0.09212478 0.10914439 0.09435597\n",
      "  0.09777236 0.09477749 0.1067787  0.09898845]\n",
      " [0.10009831 0.10411804 0.10173165 0.09246823 0.10912023 0.09447601\n",
      "  0.09783275 0.09463181 0.10665732 0.09886566]\n",
      " [0.10027172 0.10440649 0.10181734 0.09232815 0.10912602 0.09440168\n",
      "  0.09761182 0.09447828 0.10699802 0.09856049]\n",
      " [0.09987836 0.10455172 0.1015056  0.09254825 0.10918437 0.09433474\n",
      "  0.09813606 0.09444878 0.1065923  0.09881982]\n",
      " [0.09991426 0.1045997  0.10180058 0.09231351 0.10938338 0.09461877\n",
      "  0.09779164 0.0943223  0.1067135  0.09854235]\n",
      " [0.10001649 0.10433141 0.10182688 0.09239803 0.10906232 0.09478203\n",
      "  0.09780413 0.09456332 0.10671561 0.09849979]\n",
      " [0.09998674 0.10430497 0.10164664 0.0924981  0.10928808 0.09490783\n",
      "  0.09790301 0.09442147 0.10665137 0.09839179]\n",
      " [0.09985571 0.10419787 0.10154588 0.09232744 0.1093726  0.09443583\n",
      "  0.09760065 0.09456743 0.10714128 0.09895531]\n",
      " [0.1000408  0.10387581 0.10172546 0.09248383 0.1091284  0.09472233\n",
      "  0.09788928 0.09448819 0.10682727 0.09881863]\n",
      " [0.10001113 0.1044194  0.10143023 0.09223439 0.10912861 0.09494229\n",
      "  0.09787358 0.09462226 0.1066977  0.0986404 ]\n",
      " [0.10000343 0.10416671 0.10173693 0.09261263 0.1092169  0.0947864\n",
      "  0.09776095 0.09435633 0.10662551 0.0987342 ]\n",
      " [0.10018305 0.10440296 0.10124034 0.09221078 0.10917005 0.09461457\n",
      "  0.09819733 0.0944611  0.10683163 0.09868818]\n",
      " [0.10000755 0.10418743 0.10157823 0.09265839 0.10937261 0.09485909\n",
      "  0.09789365 0.09442222 0.1067405  0.09828033]\n",
      " [0.10007609 0.10470127 0.10173695 0.09237002 0.10886114 0.0946583\n",
      "  0.09777903 0.09433285 0.10668919 0.09879515]\n",
      " [0.10005548 0.10458335 0.10148675 0.09204701 0.10922932 0.09479467\n",
      "  0.09808129 0.09442151 0.10671613 0.09858449]\n",
      " [0.10019496 0.10413822 0.10160054 0.09252521 0.10903952 0.09480431\n",
      "  0.09796091 0.09426909 0.10665794 0.0988093 ]\n",
      " [0.1001909  0.10420379 0.10160769 0.09228567 0.10892344 0.09473709\n",
      "  0.09793761 0.09445341 0.10659166 0.09906874]\n",
      " [0.09996629 0.10424345 0.10156791 0.09249088 0.1088966  0.09480475\n",
      "  0.09782712 0.09466234 0.10674468 0.09879598]\n",
      " [0.09980489 0.10434802 0.10177641 0.09273831 0.10931895 0.09436876\n",
      "  0.09771867 0.09427048 0.10676755 0.09888796]\n",
      " [0.09965043 0.10458648 0.10152714 0.09269034 0.1091354  0.0945226\n",
      "  0.09790085 0.09446741 0.10671413 0.09880522]\n",
      " [0.10002714 0.10459752 0.10151505 0.0920923  0.10936033 0.09477078\n",
      "  0.09774401 0.09443671 0.1068005  0.09865566]\n",
      " [0.10024942 0.10421882 0.10151863 0.09225695 0.10908941 0.09482925\n",
      "  0.09781678 0.09452251 0.10673039 0.09876783]\n",
      " [0.10007231 0.10459378 0.1014952  0.09248858 0.10913722 0.0945629\n",
      "  0.09741753 0.09445557 0.10709766 0.09867925]\n",
      " [0.10018139 0.10439027 0.10160974 0.09198404 0.10879111 0.09487006\n",
      "  0.09795555 0.09435264 0.10699929 0.09886591]\n",
      " [0.10024736 0.104357   0.10164738 0.09228985 0.10916721 0.09452717\n",
      "  0.09791817 0.09434659 0.10687336 0.09862591]\n",
      " [0.10005415 0.10434769 0.10148704 0.09290124 0.10914992 0.0945614\n",
      "  0.09765533 0.0945188  0.10657054 0.0987539 ]\n",
      " [0.09998657 0.1043434  0.10165627 0.09216665 0.10952419 0.09441293\n",
      "  0.09790866 0.09450242 0.10681283 0.09868608]\n",
      " [0.10013151 0.10414743 0.10173693 0.09257061 0.10908367 0.09481293\n",
      "  0.0977161  0.09460615 0.10668922 0.09850545]\n",
      " [0.09997555 0.10412832 0.10162591 0.09227936 0.10919735 0.09450207\n",
      "  0.09791737 0.09476504 0.10680187 0.09880717]\n",
      " [0.10022616 0.10442042 0.10155499 0.09254627 0.10926083 0.09440826\n",
      "  0.09773698 0.09414552 0.10700158 0.098699  ]\n",
      " [0.10016058 0.10453664 0.10174016 0.09221825 0.10914856 0.09472798\n",
      "  0.09781821 0.09433617 0.1068178  0.09849566]\n",
      " [0.09992124 0.10466773 0.10167322 0.09225327 0.10894497 0.09452494\n",
      "  0.09802094 0.09475954 0.10641659 0.09881755]\n",
      " [0.10011335 0.10429928 0.10154538 0.09269857 0.108812   0.09457311\n",
      "  0.09786151 0.0943628  0.10707534 0.09865866]\n",
      " [0.09989641 0.10432005 0.10153624 0.09240285 0.10930458 0.09473904\n",
      "  0.09776574 0.09457721 0.10643021 0.09902767]\n",
      " [0.09984569 0.10443784 0.10164088 0.09254646 0.10907495 0.09495409\n",
      "  0.09751751 0.09428707 0.10686309 0.09883243]\n",
      " [0.10018283 0.10459778 0.1016141  0.09251951 0.10878243 0.09455172\n",
      "  0.09791865 0.09423076 0.10697153 0.09863069]\n",
      " [0.10031506 0.10412366 0.10161239 0.09279857 0.10923706 0.09466492\n",
      "  0.0976395  0.0943287  0.10652819 0.09875195]\n",
      " [0.09995315 0.10456303 0.10158516 0.0923558  0.10888419 0.09498274\n",
      "  0.09761713 0.09473469 0.10620265 0.09912147]\n",
      " [0.10011641 0.10420962 0.1013993  0.09256696 0.10956903 0.09456923\n",
      "  0.09763572 0.09439221 0.10688818 0.09865332]\n",
      " [0.10039853 0.10399729 0.10204273 0.0922165  0.10894242 0.0941867\n",
      "  0.09776917 0.09475501 0.10712579 0.09856586]\n",
      " [0.10043715 0.1045001  0.10181301 0.09218368 0.10942718 0.0947569\n",
      "  0.0977407  0.09431102 0.1065126  0.09831764]\n",
      " [0.09991874 0.10475009 0.10168256 0.09244392 0.10924621 0.09458824\n",
      "  0.09786433 0.09398803 0.10678903 0.09872885]\n",
      " [0.10000983 0.10432627 0.10164728 0.09246108 0.10923194 0.09464253\n",
      "  0.09800759 0.0944862  0.10686047 0.09832682]\n",
      " [0.09986849 0.10461976 0.10158556 0.0922288  0.10907093 0.09493486\n",
      "  0.09779167 0.09459898 0.1067422  0.09855875]\n",
      " [0.10020273 0.10421099 0.10185502 0.09267309 0.10893117 0.09449997\n",
      "  0.09783229 0.09427346 0.10675602 0.09876527]\n",
      " [0.09980333 0.10458838 0.10154993 0.09265112 0.10917783 0.09469694\n",
      "  0.09801795 0.0943972  0.1064015  0.09871583]\n",
      " [0.10012488 0.10451036 0.10180071 0.09257905 0.10922337 0.09449372\n",
      "  0.09763671 0.09437563 0.10659375 0.09866183]\n",
      " [0.10003384 0.10461359 0.10178915 0.09235653 0.10887492 0.09461974\n",
      "  0.09791535 0.09442207 0.10687249 0.09850232]\n",
      " [0.09968384 0.10418747 0.10161162 0.09270071 0.10901165 0.09455805\n",
      "  0.09782091 0.09465577 0.10685746 0.09891254]\n",
      " [0.09989506 0.10467446 0.10178354 0.09240004 0.10897591 0.09474835\n",
      "  0.09792258 0.09446044 0.10679014 0.09834948]\n",
      " [0.10017995 0.10462502 0.10184737 0.09255592 0.10884455 0.09474013\n",
      "  0.09789987 0.09419907 0.10627498 0.09883312]\n",
      " [0.10008889 0.10458555 0.10156364 0.09230814 0.10908053 0.09467543\n",
      "  0.09753255 0.09453256 0.10688848 0.09874425]\n",
      " [0.09970049 0.10390184 0.10193059 0.09268893 0.10919861 0.09453783\n",
      "  0.09771408 0.09460762 0.10710511 0.09861489]\n",
      " [0.09997097 0.10429254 0.10175988 0.09230483 0.10884014 0.09449183\n",
      "  0.09784924 0.09467723 0.10698195 0.09883138]\n",
      " [0.09980221 0.10417631 0.10171859 0.09232433 0.1089712  0.09444685\n",
      "  0.09790568 0.09492541 0.10639433 0.09933508]\n",
      " [0.10031048 0.10440628 0.10171926 0.09214883 0.10918153 0.09454389\n",
      "  0.09794907 0.09420603 0.10696646 0.09856818]\n",
      " [0.10031346 0.10430381 0.10171994 0.09224664 0.1090932  0.09481345\n",
      "  0.09762572 0.09431539 0.10675172 0.09881667]\n",
      " [0.1001614  0.10418435 0.10170413 0.09248967 0.10894322 0.09451917\n",
      "  0.09802876 0.09427701 0.10693566 0.09875663]\n",
      " [0.10010275 0.1041696  0.10171818 0.09238697 0.10908056 0.09482517\n",
      "  0.09769582 0.09463328 0.10664636 0.09874132]\n",
      " [0.10007909 0.10440953 0.10175991 0.09253187 0.1093674  0.09438163\n",
      "  0.0974384  0.09436169 0.10686792 0.09880257]\n",
      " [0.09984981 0.10434997 0.10159952 0.09253397 0.10929873 0.09460257\n",
      "  0.09753295 0.09475858 0.10678621 0.09868768]\n",
      " [0.09975829 0.10430775 0.10168874 0.09258954 0.10897346 0.09457327\n",
      "  0.09787213 0.09452981 0.10675197 0.09895503]\n",
      " [0.10016438 0.1042737  0.10188012 0.09220589 0.10912792 0.09462915\n",
      "  0.09775117 0.09451195 0.10686728 0.09858845]\n",
      " [0.09969315 0.10418257 0.10150139 0.09255697 0.10921668 0.09458232\n",
      "  0.09767014 0.09477455 0.10700028 0.09882194]\n",
      " [0.10014253 0.10455776 0.10167317 0.0924859  0.10899684 0.09458563\n",
      "  0.09773135 0.09447709 0.10675874 0.09859098]\n",
      " [0.10012208 0.10448898 0.10155965 0.09228221 0.10912702 0.09465083\n",
      "  0.09779604 0.09458497 0.10667643 0.09871178]\n",
      " [0.10002518 0.10400301 0.10180007 0.0925548  0.10937848 0.09449541\n",
      "  0.09785644 0.09461844 0.10683839 0.09842977]\n",
      " [0.10002234 0.10446428 0.10164722 0.09221401 0.10937444 0.09471301\n",
      "  0.09757766 0.0945046  0.10718871 0.09829372]\n",
      " [0.10015497 0.10425594 0.10142605 0.09240449 0.10933904 0.09477586\n",
      "  0.09790356 0.09436939 0.10688816 0.09848255]\n",
      " [0.10045188 0.10424028 0.10153679 0.09263694 0.10892405 0.09452154\n",
      "  0.09795018 0.09435388 0.10677986 0.0986046 ]\n",
      " [0.0999811  0.10441142 0.10162979 0.0924083  0.10925159 0.09476154\n",
      "  0.09778913 0.09435179 0.10673098 0.09868437]\n",
      " [0.10002723 0.10444143 0.10187856 0.09219267 0.10908993 0.09469691\n",
      "  0.09788077 0.09436056 0.10668825 0.09874371]\n",
      " [0.09996442 0.10462512 0.10167606 0.09269564 0.10913569 0.09448475\n",
      "  0.09790701 0.09418777 0.10653712 0.0987864 ]\n",
      " [0.09985237 0.10399612 0.10191136 0.0924642  0.10903436 0.09444073\n",
      "  0.09799638 0.0945401  0.1067268  0.09903758]\n",
      " [0.09992787 0.10399385 0.10176221 0.09245359 0.10933818 0.09442942\n",
      "  0.09815747 0.0945293  0.10686481 0.0985433 ]\n",
      " [0.10024484 0.10445972 0.10182149 0.09211623 0.10925432 0.09456106\n",
      "  0.09771122 0.09426332 0.10681859 0.09874921]\n",
      " [0.10000487 0.10419949 0.10155727 0.09252983 0.10932591 0.09434357\n",
      "  0.0979655  0.09455562 0.1067193  0.09879865]\n",
      " [0.10002733 0.10417812 0.1016899  0.09222268 0.10915364 0.0946812\n",
      "  0.09752371 0.09492159 0.1069162  0.09868563]\n",
      " [0.10031392 0.10440969 0.10175178 0.09248694 0.10909778 0.09463938\n",
      "  0.09791932 0.09412114 0.10683607 0.09842399]\n",
      " [0.09993596 0.1042818  0.10187656 0.09250028 0.10883308 0.09445255\n",
      "  0.0979519  0.09434287 0.10693719 0.09888781]\n",
      " [0.1001377  0.10435472 0.10147878 0.09267606 0.10935254 0.09467493\n",
      "  0.09745735 0.09423138 0.1071213  0.09851524]\n",
      " [0.09980831 0.10426622 0.10149869 0.09269307 0.10917204 0.09466158\n",
      "  0.09750786 0.09488311 0.10692733 0.09858179]\n",
      " [0.09976278 0.10400436 0.10163139 0.09236537 0.10912348 0.09493478\n",
      "  0.09799125 0.09449629 0.10680641 0.0988839 ]\n",
      " [0.0999359  0.10412603 0.10174755 0.09228698 0.10889316 0.09446831\n",
      "  0.09799647 0.09498299 0.10683559 0.09872701]\n",
      " [0.09997736 0.10446546 0.10159808 0.09231063 0.10934285 0.09470329\n",
      "  0.09749576 0.09445419 0.10702485 0.09862754]\n",
      " [0.10011305 0.1044618  0.10173464 0.09204428 0.1090155  0.09470739\n",
      "  0.0980642  0.09443695 0.1069078  0.09851439]\n",
      " [0.10002164 0.10476892 0.10171191 0.09255295 0.10865766 0.09441337\n",
      "  0.09810642 0.0943437  0.10654311 0.0988803 ]\n",
      " [0.09990486 0.10440148 0.10183188 0.09254912 0.109347   0.09441468\n",
      "  0.0977008  0.09448294 0.10686001 0.09850725]\n",
      " [0.09979408 0.10457934 0.10130015 0.09270627 0.10907335 0.0945797\n",
      "  0.09817807 0.09460605 0.10647306 0.09870992]\n",
      " [0.09978373 0.10402685 0.10155134 0.0926148  0.10893484 0.09475453\n",
      "  0.09796968 0.09478953 0.10660032 0.09897438]\n",
      " [0.10000758 0.10463996 0.1017995  0.09225466 0.10878651 0.09455949\n",
      "  0.09774511 0.09457434 0.10690355 0.0987293 ]\n",
      " [0.10010238 0.10452189 0.10170269 0.09228473 0.10907444 0.09443391\n",
      "  0.09763126 0.09464469 0.10682419 0.09877982]]\n"
     ]
    }
   ],
   "source": [
    "# 伪造100个训练样本\n",
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算梯度是一个比较贵的事情\n",
    "x = np.random.rand(100, 784)\n",
    "# 伪装正确的100个标签分布\n",
    "t = np.random.rand(100, 10)\n",
    "grads = net.numerical_gradient(x, t)\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b1'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.14895782e-05 -5.94896177e-05  1.76601753e-04 ...  5.19010634e-05\n",
      "   1.32204294e-04  8.43112269e-05]\n",
      " [-5.93863914e-05 -4.49271398e-05  3.15169535e-05 ...  4.39035386e-05\n",
      "   8.84312756e-05  8.41148040e-05]\n",
      " [-3.60853769e-05 -1.69683230e-04  2.29160713e-04 ...  3.54999341e-05\n",
      "   1.35931766e-04  1.79402864e-04]\n",
      " ...\n",
      " [-5.17493359e-05 -6.21892537e-05  1.85132569e-04 ...  1.43498151e-04\n",
      "   1.37354683e-04  1.01916753e-04]\n",
      " [-9.06201469e-05 -1.27050435e-04  9.77114922e-05 ...  2.24884489e-05\n",
      "   1.45765595e-04  5.63716185e-05]\n",
      " [-8.29183788e-05 -6.84817314e-06 -1.51867185e-06 ...  3.65391828e-05\n",
      "   6.07117623e-05 -1.09337450e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(grads['W1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini-batch的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 将特征空间拉平，将标签转换成one-hot编码\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "t_train = np_utils.to_categorical(t_train, 10)\n",
    "t_test = np_utils.to_categorical(t_test, 10)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/y/Documents/code/pythonDlBasic/4.ipynb Cell 45'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000045?line=12'>13</a>\u001b[0m t_batch \u001b[39m=\u001b[39m t_train[batch_mask]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000045?line=13'>14</a>\u001b[0m \u001b[39m# 计算梯度\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000045?line=14'>15</a>\u001b[0m grad \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mnumerical_gradient(x_batch, t_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000045?line=15'>16</a>\u001b[0m \u001b[39m# 更新参数\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000045?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb2\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;32m/home/y/Documents/code/pythonDlBasic/4.ipynb Cell 37'\u001b[0m in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=26'>27</a>\u001b[0m loss_W \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=28'>29</a>\u001b[0m grads \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=29'>30</a>\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m numerical_gradient(loss_W, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mW1\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=30'>31</a>\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m numerical_gradient(loss_W, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=31'>32</a>\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m numerical_gradient(loss_W, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32m/home/y/Documents/code/pythonDlBasic/4.ipynb Cell 36'\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000046?line=43'>44</a>\u001b[0m fxh1 \u001b[39m=\u001b[39m f(x) \u001b[39m# f(x+h)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000046?line=45'>46</a>\u001b[0m x[idx] \u001b[39m=\u001b[39m tmp_val \u001b[39m-\u001b[39m h \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000046?line=46'>47</a>\u001b[0m fxh2 \u001b[39m=\u001b[39m f(x) \u001b[39m# f(x-h)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000046?line=47'>48</a>\u001b[0m grad[idx] \u001b[39m=\u001b[39m (fxh1 \u001b[39m-\u001b[39m fxh2) \u001b[39m/\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mh)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000046?line=49'>50</a>\u001b[0m x[idx] \u001b[39m=\u001b[39m tmp_val \u001b[39m# 値を元に戻す\u001b[39;00m\n",
      "\u001b[1;32m/home/y/Documents/code/pythonDlBasic/4.ipynb Cell 37'\u001b[0m in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumerical_gradient\u001b[39m(\u001b[39mself\u001b[39m, x, t):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=26'>27</a>\u001b[0m     loss_W \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(x, t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=28'>29</a>\u001b[0m     grads \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=29'>30</a>\u001b[0m     grads[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m numerical_gradient(loss_W, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32m/home/y/Documents/code/pythonDlBasic/4.ipynb Cell 37'\u001b[0m in \u001b[0;36mTwoLayerNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, x, t):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=17'>18</a>\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cross_entropy_error(y, t)\n",
      "\u001b[1;32m/home/y/Documents/code/pythonDlBasic/4.ipynb Cell 37'\u001b[0m in \u001b[0;36mTwoLayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=9'>10</a>\u001b[0m W1, W2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=10'>11</a>\u001b[0m b1, b2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mb2\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=11'>12</a>\u001b[0m a1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(x, W1) \u001b[39m+\u001b[39m b1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=12'>13</a>\u001b[0m z1 \u001b[39m=\u001b[39m sigmoid(a1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/y/Documents/code/pythonDlBasic/4.ipynb#ch0000035?line=13'>14</a>\u001b[0m a2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(z1, W2) \u001b[39m+\u001b[39m b2\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "# 从这里就开始梯度下降了，寻找最好的参数了\n",
    "for i in range(iters_num):\n",
    "    # 获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    # 计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    # 记录下损失\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1021d31412234ebea20baf61e8fdbc8c22153a59dde70b52bcafccce89619e30"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
