{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuyixi520/code/blob/main/ernie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU6zShlL0UeD"
      },
      "source": [
        "\n",
        "# 【快速上手ERNIE 3.0】中文情感分析、法律文本多标签分类、中文语义匹配、MSRA序列标注项目实战\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zY_BIJh0UeJ"
      },
      "source": [
        "## ERNIE 3.0模型介绍\n",
        "ERNIE 3.0首次在百亿级预训练模型中引入大规模知识图谱，提出了海量无监督文本与大规模知识图谱的平行预训练方法(Universal Knowledge-Text Prediction)，通过将知识图谱挖掘算法得到五千万知识图谱三元组与4TB大规模语料同时输入到预训练模型中进行联合掩码训练，促进了结构化知识和无结构文本之间的信息共享，大幅提升了模型对于知识的记忆和推理能力。\n",
        "\n",
        "ERNIE 3.0框架分为两层。第一层是通用语义表示网络，该网络学习数据中的基础和通用的知识。第二层是任务语义表示网络，该网络基于通用语义表示，学习任务相关的知识。在学习过程中，任务语义表示网络只学习对应类别的预训练任务，而通用语义表示网络会学习所有的预训练任务。\n",
        "\n",
        "![](https://ai-studio-static-online.cdn.bcebos.com/b00b30257fcf44ad98f37e496702b6dc7a0d32ffb1de469b94f56829ed7766aa)\n",
        "<font size=2><center>ERNIE 3.0模型框架</center></font>\n",
        "\n",
        "[comment]: <> (ERNIE 3.0介绍参考新闻稿 http://ex.chinadaily.com.cn/exchange/partners/82/rss/channel/cn/columns/snl9a7/stories/WS60e41d0fa3101e7ce9758648.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-17T13:10:08.617749Z",
          "iopub.status.busy": "2022-06-17T13:10:08.617270Z",
          "iopub.status.idle": "2022-06-17T13:10:45.211924Z",
          "shell.execute_reply": "2022-06-17T13:10:45.210741Z",
          "shell.execute_reply.started": "2022-06-17T13:10:08.617639Z"
        },
        "scrolled": true,
        "id": "Aq2M2tlh0UeK",
        "outputId": "b0a7bc22-b122-497a-b3e9-0c9a61451c03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Collecting paddlenlp\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7a/31/162931501c562c5db43c366643fd0d634aa41f85f6165885cd073d1721a2/paddlenlp-2.3.3-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 6.0MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.1.85)\n",
            "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
            "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
            "Collecting datasets>=2.0.0 (from paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/98/29/f381f8a633fed2c4f41c191498c3bc43d91a8e44c5202a8b0b2bd8b1acf3/datasets-2.3.2-py3-none-any.whl (362kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 9.1MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting paddle2onnx (from paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/70/cc/51680d1298d6ed14a4f88ee4fa3a27819936601a1df660e1c2f827336d9d/paddle2onnx-0.9.7-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 8.9MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting dill<0.3.5 (from paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b6/c3/973676ceb86b60835bb3978c6db67a5dc06be6cfdbd14ef0f5a13e3fc9fd/dill-0.3.4-py2.py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 53.5MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
            "Collecting paddlefsl (from paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fb/4a/25d1959a8f1fe5ee400f32fc9fc8b56d4fd6fc25315e23c0171f6e705e2a/paddlefsl-1.1.0-py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 474kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
            "Collecting multiprocess<=0.70.12.2 (from paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/aa/d8/d7bbcef5c03890f5fe983d8419b0c5236af3657c5aa9bddf1991a6ed813a/multiprocess-0.70.12.2-py37-none-any.whl (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 408kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf<=3.20.0,>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (3.11.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.62.1)\n",
            "Collecting pyarrow>=6.0.0 (from datasets>=2.0.0->paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bd/a2/2b5dcc60b4977e1d70f192d7f67ed12f038cc57c91232fc01347f0d86311/pyarrow-8.0.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (28.0MB)\n",
            "\u001b[K     |████████████████████████████████| 28.0MB 5.5MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting aiohttp (from datasets>=2.0.0->paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/dd/fe/80c594d62a7ff07730fd2cfc3a058498087436d8c938243e0610d1928f0e/aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 10.9MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting xxhash (from datasets>=2.0.0->paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/20/3e/ca49932bade8b3308e74df951c36cbc84c8230c9b8715bae1e0014831aa7/xxhash-3.0.0.tar.gz (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.5MB/s eta 0:00:011\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (1.20.3)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (0.23)\n",
            "Collecting fsspec[http]>=2021.05.0 (from datasets>=2.0.0->paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bd/4c/166d788feff5c739b833342945bbba406581095fb6c4a056113fae646b5c/fsspec-2022.5.0-py3-none-any.whl (140kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 55.5MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (21.3)\n",
            "Collecting responses<0.19 (from datasets>=2.0.0->paddlenlp)\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/79/f3/2b3a6dc5986303b3dd1bbbcf482022acb2583c428cd23f0b6d37b1a1a519/responses-0.18.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (2.22.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0 (from datasets>=2.0.0->paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c3/dd/19bbd206cb893c5a668b3e79f6b0227a9c07bfb808b9b2415b00c77bc79b/huggingface_hub-0.8.0-py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 2.9MB/s eta 0:00:011\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.22.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from protobuf<=3.20.0,>=3.1.0->paddlenlp) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from protobuf<=3.20.0,>=3.1.0->paddlenlp) (41.4.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.0.0->paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fa/a7/71c253cdb8a1528802bac7503bf82fe674367e4055b09c28846fdfa4ab90/multidict-6.0.2.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.1MB/s eta 0:00:011\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting asynctest==0.13.0; python_version < \"3.8\" (from aiohttp->datasets>=2.0.0->paddlenlp)\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e8/b6/8d17e169d577ca7678b11cd0d3ceebb0a6089a7f4a2de4b945fe4b1c86db/asynctest-0.13.0-py3-none-any.whl\n",
            "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets>=2.0.0->paddlenlp)\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d6/c1/8991e7c5385b897b8c020cdaad718c5b087a6626d1d11a23e1ea87e325a7/async_timeout-4.0.2-py3-none-any.whl\n",
            "Collecting typing-extensions>=3.7.4; python_version < \"3.8\" (from aiohttp->datasets>=2.0.0->paddlenlp)\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/75/e1/932e06004039dd670c9d5e1df0cd606bf46e29a28e65d5bb28e894ea29c9/typing_extensions-4.2.0-py3-none-any.whl\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->paddlenlp)\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3b/87/fe94898f2d44a93a35d5aa74671ed28094d80753a1113d68b799fab6dc22/aiosignal-1.2.0-py3-none-any.whl\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.0.0->paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d1/ae/e4437fe5b5ba0fbccdaf8ecde8e3b6e8903793ca638c4706d034c0969ce1/frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 4.5MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (19.2.0)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.0.0->paddlenlp)\n",
            "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/80/7f/af3ecdf87e8e41da7b133f1d61f82745f8c862bdade3b56addee3ad23956/yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 3.8MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting charset-normalizer<3.0,>=2.0 (from aiohttp->datasets>=2.0.0->paddlenlp)\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/06/b3/24afc8868eba069a7f03650ac750a778862dc34941a4bebeb58706715726/charset_normalizer-2.0.12-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=2.0.0->paddlenlp) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->datasets>=2.0.0->paddlenlp) (2.4.2)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.25.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from responses<0.19->datasets>=2.0.0->paddlenlp) (1.25.10)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp) (2019.3)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (2019.9.11)\n",
            "Collecting filelock (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp)\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a6/d5/17f02b379525d1ff9678bfa58eb9548f561c8826deb0b85797aa0eed582d/filelock-3.7.1-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp) (5.1.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->datasets>=2.0.0->paddlenlp) (7.2.0)\n",
            "Building wheels for collected packages: xxhash, multidict\n",
            "  Building wheel for xxhash (PEP 517) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for xxhash: filename=xxhash-3.0.0-cp37-cp37m-linux_x86_64.whl size=142544 sha256=6fd259e7ecc70623e94c928a176f48b145764737ccea2236773d78d9f84c8c77\n",
            "  Stored in directory: /home/work/.cache/pip/wheels/d7/73/d0/a24ef3d1b31c775fd0a0ddd44f6b4beb6dcd249f5f258f0f10\n",
            "  Building wheel for multidict (PEP 517) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for multidict: filename=multidict-6.0.2-cp37-cp37m-linux_x86_64.whl size=125985 sha256=99604246c0edc09b8744d3cf384239f5fdb0b801c3168ec8864fd6e26846ddb6\n",
            "  Stored in directory: /home/work/.cache/pip/wheels/e9/05/c9/b32ddd33fa853c5f946de58000a326db3b5de29350c1037148\n",
            "Successfully built xxhash multidict\n",
            "\u001b[31mERROR: parl 1.1.2 has requirement pyarrow==0.13.0, but you'll have pyarrow 8.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyarrow, dill, multidict, asynctest, typing-extensions, async-timeout, frozenlist, aiosignal, yarl, charset-normalizer, aiohttp, xxhash, multiprocess, fsspec, responses, filelock, huggingface-hub, datasets, paddle2onnx, paddlefsl, paddlenlp\n",
            "  Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 charset-normalizer-2.0.12 datasets-2.3.2 dill-0.3.4 filelock-3.7.1 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.8.0 multidict-6.0.2 multiprocess-0.70.12.2 paddle2onnx-0.9.7 paddlefsl-1.1.0 paddlenlp-2.3.3 pyarrow-8.0.0 responses-0.18.0 typing-extensions-4.2.0 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
            "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/scipy/linalg/__init__.py:217: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
            "  from numpy.dual import register_func\n"
          ]
        }
      ],
      "source": [
        "#基础环境准备\n",
        "!pip install --upgrade paddlenlp\n",
        "\n",
        "import os\n",
        "import paddle\n",
        "import paddlenlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhzkPM0D0UeN"
      },
      "source": [
        "# 一. 中文情感分析实战\n",
        "本项目将介绍如何基于PaddleNLP利用ERNIE 3.0预训练模型微调并进行中文情感分析预测，主要包括“什么是情感分析任务”、“ERNIE 3.0模型”、“如何使用ERNIE 3.0中文预训练模型进行句子级别情感分析”等三个部分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNGzgy_70UeO"
      },
      "source": [
        "\n",
        "## 1. 什么是情感分析任务\n",
        "\n",
        "人类的自然语言蕴含着丰富的情感色彩，语言可以表达情绪（如悲伤、快乐）、心情（如倦怠、忧郁）、喜好（如喜欢、讨厌）、个性特征和立场等等。在互联网大数据时代，人类比以往任何时候都更公开地表达自己的想法和感受，如何快速地监控和理解所有类型数据中情绪变得尤为重要。情感分析是一种自然语言处理 (NLP) 技术，用于确定数据情感是正面的、负面的还是中性的。情感分析通常在文本数据上进行，在商品喜好、消费决策、舆情分析等场景中均有应用。利用机器自动分析这些情感倾向，不但有助于帮助企业监控客户反馈中的品牌和产品情感，并了解客户需求，还有助于企业分析商业伙伴们的态度，以便更好地进行商业决策。\n",
        "\n",
        "生活中常见将一句话或一段文字的进行情感标记，如标记为正向、负向、中性的三分类问题，这属于句子级别情感分析任务。此外常见的情感分析任务还包括词级别情感分析和目标级别情感分析。\n",
        "\n",
        "![](https://ai-studio-static-online.cdn.bcebos.com/49480ab6429b45049faf22febd0abace2cabda1630574cfb990c4fef4fecd453)\n",
        "<font size=2><center>句子的情感被标记为正向、负向或中性</center></font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4FATIYV0UeP"
      },
      "source": [
        "\n",
        "## 2. 如何使用ERNIE 3.0中文预训练模型进行句子级别情感分析\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw9fnrI20UeP"
      },
      "source": [
        "\n",
        "### 2.1 加载中文情感分析数据集ChnSentiCorp\n",
        "\n",
        "ChnSentiCorp是中文句子级情感分类数据集，包含酒店、笔记本电脑和书籍的网购评论，数据集示例：\n",
        "```\n",
        " qid\tlabel\ttext_a\n",
        " 0\t1\t這間酒店環境和服務態度亦算不錯,但房間空間太小~~不宣容納太大件行李~~且房間格調還可以~~ 中餐廳的廣東點心不太好吃~~要改善之~~~~但算價錢平宜~~可接受~~ 西餐廳格調都很好~~但吃的味道一般且令人等得太耐了~~要改善之~~\n",
        " 1\t<荐书> 推荐所有喜欢<红楼>的红迷们一定要收藏这本书,要知道当年我听说这本书的时候花很长时间去图书馆找和借都没能如愿,所以这次一看到当当有,马上买了,红迷们也要记得备货哦!\n",
        " 2\t0\t商品的不足暂时还没发现，京东的订单处理速度实在.......周二就打包完成，周五才发货...\n",
        " ...\n",
        "```\n",
        "其中1表示正向情感，0表示负向情感，PaddleNLP已经内置该数据集，一键即可加载。更多数据集自定方法详见[如何自定义数据集](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_self_defined.html)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-17T13:11:36.069683Z",
          "iopub.status.busy": "2022-06-17T13:11:36.069190Z",
          "iopub.status.idle": "2022-06-17T13:11:36.665437Z",
          "shell.execute_reply": "2022-06-17T13:11:36.664522Z",
          "shell.execute_reply.started": "2022-06-17T13:11:36.069621Z"
        },
        "scrolled": true,
        "tags": [],
        "id": "YB06qjx00UeQ",
        "outputId": "1185dd4a-7993-47b1-9b2c-40ed9dd89725"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1909/1909 [00:00<00:00, 5568.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "数据类型: <class 'paddlenlp.datasets.dataset.MapDataset'>\n",
            "训练集样例: {'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', 'label': 1, 'qid': ''}\n",
            "验证集样例: {'text': '這間酒店環境和服務態度亦算不錯,但房間空間太小~~不宣容納太大件行李~~且房間格調還可以~~ 中餐廳的廣東點心不太好吃~~要改善之~~~~但算價錢平宜~~可接受~~ 西餐廳格調都很好~~但吃的味道一般且令人等得太耐了~~要改善之~~', 'label': 1, 'qid': '0'}\n",
            "测试集样例: {'text': '这个宾馆比较陈旧了，特价的房间也很一般。总体来说一般', 'label': '', 'qid': '0'}\n"
          ]
        }
      ],
      "source": [
        "#加载中文评论情感分析语料数据集ChnSentiCorp\n",
        "from paddlenlp.datasets import load_dataset\n",
        "\n",
        "train_ds, dev_ds, test_ds = load_dataset(\"chnsenticorp\", splits=[\"train\", \"dev\", \"test\"])\n",
        "\n",
        "# 数据集返回为MapDataset类型\n",
        "print(\"数据类型:\", type(train_ds))\n",
        "# label代表标签，qid代表数据编号，测试集中不包含标签信息\n",
        "print(\"训练集样例:\", train_ds[0])\n",
        "print(\"验证集样例:\", dev_ds[0])\n",
        "print(\"测试集样例:\", test_ds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za0xe0Ct0UeR"
      },
      "source": [
        "### 2.2 加载中文ERNIE 3.0预训练模型和分词器\n",
        "\n",
        "PaddleNLP中Auto模块（包括AutoModel, AutoTokenizer及各种下游任务类）提供了方便易用的接口，无需指定模型类别，即可调用不同网络结构的预训练模型。PaddleNLP的预训练模型可以很容易地通过from_pretrained()方法加载，[Transformer预训练模型汇总](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html#transformer)包含了40多个主流预训练模型，500多个模型权重。\n",
        "\n",
        "\n",
        "AutoModelForSequenceClassification可用于句子级情感分析和目标级情感分析任务，通过预训练模型获取输入文本的表示，之后将文本表示进行分类。PaddleNLP已经实现了ERNIE 3.0预训练模型，可以通过一行代码实现ERNIE 3.0预训练模型和分词器的加载。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-17T13:12:43.380878Z",
          "iopub.status.busy": "2022-06-17T13:12:43.380424Z",
          "iopub.status.idle": "2022-06-17T13:13:09.752483Z",
          "shell.execute_reply": "2022-06-17T13:13:09.751708Z",
          "shell.execute_reply.started": "2022-06-17T13:12:43.380825Z"
        },
        "scrolled": true,
        "tags": [],
        "id": "QLEN7eBy0UeS",
        "outputId": "686fe494-8978-45dd-ca0a-e111c3af24bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2022-06-17 21:12:43,382] [    INFO] - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-base-zh'.\n",
            "[2022-06-17 21:12:43,385] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh.pdparams and saved to /home/work/.paddlenlp/models/ernie-3.0-base-zh\n",
            "[2022-06-17 21:12:43,388] [    INFO] - Downloading ernie_3.0_base_zh.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh.pdparams\n",
            "100%|██████████| 452M/452M [00:12<00:00, 37.3MB/s] \n",
            "[2022-06-17 21:13:09,519] [    INFO] - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-base-zh'.\n",
            "[2022-06-17 21:13:09,522] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh_vocab.txt and saved to /home/work/.paddlenlp/models/ernie-3.0-base-zh\n",
            "[2022-06-17 21:13:09,524] [    INFO] - Downloading ernie_3.0_base_zh_vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh_vocab.txt\n",
            "100%|██████████| 182k/182k [00:00<00:00, 2.51MB/s]\n"
          ]
        }
      ],
      "source": [
        "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"ernie-3.0-base-zh\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=len(train_ds.label_list))\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqggvLam0UeT"
      },
      "source": [
        "\n",
        "### 2.3 基于预训练模型的数据处理\n",
        "`Dataset`中通常为原始数据，需要经过一定的数据处理并进行采样组batch。\n",
        "* 通过`Dataset`的`map`函数，使用分词器将数据集从原始文本处理成模型的输入。\n",
        "* 定义`paddle.io.BatchSampler`和`collate_fn`构建 `paddle.io.DataLoader`。\n",
        "\n",
        "实际训练中，根据显存大小调整批大小`batch_size`和文本最大长度`max_seq_length`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-17T13:13:27.299348Z",
          "iopub.status.busy": "2022-06-17T13:13:27.298223Z",
          "iopub.status.idle": "2022-06-17T13:13:27.307516Z",
          "shell.execute_reply": "2022-06-17T13:13:27.306770Z",
          "shell.execute_reply.started": "2022-06-17T13:13:27.299267Z"
        },
        "scrolled": true,
        "tags": [],
        "id": "HfM3ciKB0UeU"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import numpy as np\n",
        "\n",
        "from paddle.io import DataLoader, BatchSampler\n",
        "from paddlenlp.data import DataCollatorWithPadding\n",
        "\n",
        "# 数据预处理函数，利用分词器将文本转化为整数序列\n",
        "def preprocess_function(examples, tokenizer, max_seq_length, is_test=False):\n",
        "\n",
        "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
        "    if not is_test:\n",
        "        result[\"labels\"] = examples[\"label\"]\n",
        "    return result\n",
        "\n",
        "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=128)\n",
        "train_ds = train_ds.map(trans_func)\n",
        "dev_ds = dev_ds.map(trans_func)\n",
        "\n",
        "# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
        "collate_fn = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
        "train_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\n",
        "dev_batch_sampler = BatchSampler(dev_ds, batch_size=64, shuffle=False)\n",
        "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
        "dev_data_loader = DataLoader(dataset=dev_ds, batch_sampler=dev_batch_sampler, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9PATkGC0UeV"
      },
      "source": [
        "\n",
        "### 2.4 数据训练和评估\n",
        "定义训练所需的优化器、损失函数、评价指标等，就可以开始进行预模型微调任务。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-17T13:13:32.368678Z",
          "iopub.status.busy": "2022-06-17T13:13:32.367607Z",
          "iopub.status.idle": "2022-06-17T13:13:32.373941Z",
          "shell.execute_reply": "2022-06-17T13:13:32.373105Z",
          "shell.execute_reply.started": "2022-06-17T13:13:32.368619Z"
        },
        "scrolled": true,
        "tags": [],
        "id": "QmK0R2aB0UeV"
      },
      "outputs": [],
      "source": [
        "# Adam优化器、交叉熵损失函数、accuracy评价指标\n",
        "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())\n",
        "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
        "metric = paddle.metric.Accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-17T13:13:35.706980Z",
          "iopub.status.busy": "2022-06-17T13:13:35.705920Z"
        },
        "scrolled": true,
        "tags": [],
        "id": "c6qqN8Sy0UeW"
      },
      "outputs": [],
      "source": [
        "# 开始训练\n",
        "import time\n",
        "import paddle.nn.functional as F\n",
        "\n",
        "from demo.emotionAnalysisCn.eval import evaluate\n",
        "\n",
        "epochs = 5 # 训练轮次\n",
        "ckpt_dir = \"/home/work/PretrainedModel/emotionAnalysisCn\" #训练过程中保存模型参数的文件夹\n",
        "best_acc = 0\n",
        "best_step = 0\n",
        "global_step = 0 #迭代次数\n",
        "tic_train = time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for step, batch in enumerate(train_data_loader, start=1):\n",
        "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
        "\n",
        "        # 计算模型输出、损失函数值、分类概率值、准确率\n",
        "        logits = model(input_ids, token_type_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        probs = F.softmax(logits, axis=1)\n",
        "        correct = metric.compute(probs, labels)\n",
        "        metric.update(correct)\n",
        "        acc = metric.accumulate()\n",
        "\n",
        "        # 每迭代10次，打印损失函数值、准确率、计算速度\n",
        "        global_step += 1\n",
        "        if global_step % 10 == 0:\n",
        "            print(\n",
        "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
        "                % (global_step, epoch, step, loss, acc,\n",
        "                    10 / (time.time() - tic_train)))\n",
        "            tic_train = time.time()\n",
        "        \n",
        "        # 反向梯度回传，更新参数\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.clear_grad()\n",
        "\n",
        "        # 每迭代100次，评估当前训练的模型、保存当前模型参数和分词器的词表等\n",
        "        if global_step % 100 == 0:\n",
        "            save_dir = ckpt_dir\n",
        "            if not os.path.exists(save_dir):\n",
        "                os.makedirs(save_dir)\n",
        "            print(global_step, end=' ')\n",
        "            acc_eval = evaluate(model, criterion, metric, dev_data_loader)\n",
        "            if acc_eval > best_acc:\n",
        "                best_acc = acc_eval\n",
        "                best_step = global_step\n",
        "\n",
        "                model.save_pretrained(save_dir)\n",
        "                tokenizer.save_pretrained(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XtwCLNT0UeX"
      },
      "source": [
        "模型训练过程中会输出如下日志:\n",
        "```\n",
        "global step 10, epoch: 1, batch: 10, loss: 0.66181, accu: 0.55000, speed: 4.53 step/s\n",
        "global step 20, epoch: 1, batch: 20, loss: 0.54043, accu: 0.60938, speed: 4.92 step/s\n",
        "global step 30, epoch: 1, batch: 30, loss: 0.42240, accu: 0.67708, speed: 4.88 step/s\n",
        "global step 40, epoch: 1, batch: 40, loss: 0.34822, accu: 0.72266, speed: 4.86 step/s\n",
        "global step 50, epoch: 1, batch: 50, loss: 0.31792, accu: 0.74438, speed: 4.85 step/s\n",
        "global step 60, epoch: 1, batch: 60, loss: 0.36544, accu: 0.76719, speed: 4.86 step/s\n",
        "global step 70, epoch: 1, batch: 70, loss: 0.19064, accu: 0.78795, speed: 4.87 step/s\n",
        "global step 80, epoch: 1, batch: 80, loss: 0.32033, accu: 0.79883, speed: 4.86 step/s\n",
        "global step 90, epoch: 1, batch: 90, loss: 0.22526, accu: 0.81007, speed: 4.82 step/s\n",
        "global step 100, epoch: 1, batch: 100, loss: 0.30424, accu: 0.81781, speed: 4.85 step/s\n",
        "100 eval loss: 0.25176, accuracy: 0.91167\n",
        "[2022-05-13 17:07:09,935] [    INFO] - tokenizer config file saved in ernie_ckpt_1/tokenizer_config.json\n",
        "[2022-05-13 17:07:09,938] [    INFO] - Special tokens file saved in ernie_ckpt_1/special_tokens_map.json\n",
        "...\n",
        "```\n",
        "训练5个epoch预计需要7分钟。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "idGT7do30UeX"
      },
      "outputs": [],
      "source": [
        "from demo.emotionAnalysisCn.eval import evaluate\n",
        "# 加载ERNIR 3.0最佳模型参数\n",
        "params_path = 'PretrainedModel/emotionAnalysisCn/model_state.pdparams'\n",
        "state_dict = paddle.load(params_path)\n",
        "model.set_dict(state_dict)\n",
        "print('ERNIE 3.0 在ChnSentiCorp的dev集表现', end=' ')\n",
        "eval_acc = evaluate(model, criterion, metric, dev_data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKzi3q-m0UeX"
      },
      "source": [
        "### 2.5 情感分析结果预测与保存\n",
        "加载微调好的模型参数进行情感分析预测，并保存预测结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "9P02-f-L0UeY"
      },
      "outputs": [],
      "source": [
        "# 测试集数据预处理，利用分词器将文本转化为整数序列\n",
        "trans_func_test = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=128, is_test=True)\n",
        "test_ds_trans = test_ds.map(trans_func_test)\n",
        "\n",
        "# 进行采样组batch\n",
        "collate_fn_test = DataCollatorWithPadding(tokenizer)\n",
        "test_batch_sampler = BatchSampler(test_ds_trans, batch_size=32, shuffle=False)\n",
        "test_data_loader = DataLoader(dataset=test_ds_trans, batch_sampler=test_batch_sampler, collate_fn=collate_fn_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "Up4wGaka0UeY"
      },
      "outputs": [],
      "source": [
        "# 模型预测分类结果\n",
        "import paddle.nn.functional as F\n",
        "\n",
        "label_map = {0: '负面', 1: '正面'}\n",
        "results = []\n",
        "model.eval()\n",
        "for batch in test_data_loader:\n",
        "    input_ids, token_type_ids = batch['input_ids'], batch['token_type_ids']\n",
        "    logits = model(batch['input_ids'], batch['token_type_ids'])\n",
        "    probs = F.softmax(logits, axis=-1)\n",
        "    idx = paddle.argmax(probs, axis=1).numpy()\n",
        "    idx = idx.tolist()\n",
        "    preds = [label_map[i] for i in idx]\n",
        "    results.extend(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "LeER7UIZ0UeZ"
      },
      "outputs": [],
      "source": [
        "# 存储ChnSentiCorp预测结果  \n",
        "test_ds = load_dataset(\"chnsenticorp\", splits=[\"test\"]) \n",
        "\n",
        "res_dir = \"./results/emotionAnalysisCn\"\n",
        "if not os.path.exists(res_dir):\n",
        "    os.makedirs(res_dir)\n",
        "with open(os.path.join(res_dir, \"ChnSentiCorp.tsv\"), 'w', encoding=\"utf8\") as f:\n",
        "    f.write(\"qid\\ttext\\tprediction\\n\")\n",
        "    for i, pred in enumerate(results):\n",
        "        f.write(test_ds[i]['qid']+\"\\t\"+test_ds[i]['text']+\"\\t\"+pred+\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE3UAo2k0UeZ"
      },
      "source": [
        "ChnSentiCorp预测结果示例：\n",
        "\n",
        "![](https://ai-studio-static-online.cdn.bcebos.com/581d255179974f6fa20f588fd4dafbbc0c8c7b1b27d54de19e0d77048ed2215a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElRvXBQB0UeZ"
      },
      "source": [
        "# 二.法律文本多标签分类实战\n",
        "本项目将介绍如何基于PaddleNLP对ERNIE 3.0预训练模型微调完成法律文本多标签分类预测。本项目主要包括“什么是多标签文本分类预测”、“ERNIE 3.0模型”、“如何使用ERNIE 3.0中文预训练模型进行法律文本多标签分类预测”等三个部分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ea58zU0Uea"
      },
      "source": [
        "## 1. 什么是多标签文本分类预测  \n",
        "\n",
        "\n",
        "文本多标签分类是自然语言处理（NLP）中常见的文本分类任务，文本多标签分类在各种现实场景中具有广泛的适用性，例如商品分类、网页标签、新闻标注、蛋白质功能分类、电影分类、语义场景分类等。多标签数据集中样本用来自 `n_classes` 个可能类别的`m`个标签类别标记，其中`m`的取值在0到`n_classes`之间，这些类别具有不相互排斥的属性。通常，我们将每个样本的标签用One-hot的形式表示，正类用1表示，负类用0表示。例如，数据集中样本可能标签是A、B和C的多标签分类问题，标签为\\[1,0,1\\]代表存在标签 A 和 C 而标签 B 不存在的样本。\n",
        "\n",
        "近年来，随着司法改革的全面推进，“以公开为原则，不公开为例外”的政策逐步确立，大量包含了案件事实及其适用法律条文信息的裁判文书逐渐在互联网上公开，海量的数据使自然语言处理技术的应用成为可能。法律条文的组织呈树形层次结构，现实中的案情错综复杂，同一案件可能适用多项法律条文，涉及数罪并罚，需要多标签模型充分学习标签之间的关联性，对文本进行分类预测。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtmltfC90Uea"
      },
      "source": [
        "## 2. 如何使用ERNIE 3.0中文预训练模型进行法律文本多标签分类预测\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg0KoQMP0Uea"
      },
      "source": [
        "### 2.1 环境准备"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CHOab93t0Uea"
      },
      "outputs": [],
      "source": [
        "# 安装pyzmq\n",
        "!pip install pyzmq==18.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU6HTkcQ0Ueb"
      },
      "source": [
        "\n",
        "### 2.2 加载法律文本多标签数据\n",
        "\n",
        "本数据集（[2019年法研杯要素识别任务](https://github.com/china-ai-law-challenge/CAIL2019/tree/master/%E8%A6%81%E7%B4%A0%E8%AF%86%E5%88%AB)）来自于“中国裁判文书网”公开的法律文书，每条训练数据由一份法律文书的案情描述片段构成，其中每个句子都被标记了对应的类别标签，数据集一共包含20个标签，标签代表含义如下：\n",
        "\n",
        "```\n",
        "DV1    0    婚后有子女\n",
        "DV2    1    限制行为能力子女抚养\n",
        "DV3    2    有夫妻共同财产\n",
        "DV4    3    支付抚养费\n",
        "DV5    4    不动产分割\n",
        "DV6    5    婚后分居\n",
        "DV7    6    二次起诉离婚\n",
        "DV8    7    按月给付抚养费\n",
        "DV9    8    准予离婚\n",
        "DV10    9    有夫妻共同债务\n",
        "DV11    10    婚前个人财产\n",
        "DV12    11    法定离婚\n",
        "DV13    12    不履行家庭义务\n",
        "DV14    13    存在非婚生子\n",
        "DV15    14    适当帮助\n",
        "DV16    15    不履行离婚协议\n",
        "DV17    16    损害赔偿\n",
        "DV18    17    感情不和分居满二年\n",
        "DV19    18    子女随非抚养权人生活\n",
        "DV20    19    婚后个人财产\n",
        "```\n",
        "数据集示例：\n",
        "```\n",
        "text    labels\n",
        "所以起诉至法院请求变更两个孩子均由原告抚养，被告承担一个孩子抚养费每月600元。\t0,7,3,1\n",
        "2014年8月原、被告因感情不和分居，2014年10月16日被告文某某向务川自治县人民法院提起离婚诉讼，被法院依法驳回了离婚诉讼请求。\t6,5\n",
        "女儿由原告抚养，被告每月支付小孩抚养费500元；\t0,7,3,1\n",
        "```\n",
        "使用本地文件创建数据集，自定义`read_custom_data()`函数读取数据文件，传入`load_dataset()`创建数据集，返回数据类型为MapDataset。更多数据集自定方法详见[如何自定义数据集](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_self_defined.html)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JwuyxcQw0Ueb"
      },
      "outputs": [],
      "source": [
        "# 下载示例数据集，也可以导入数据集的方式导入数据集\n",
        "%cd /home/work/data\n",
        "!wget https://easydl-download.bj.bcebos.com/29129392321/demo_data.zip\n",
        "!unzip -q demo_data.zip\n",
        "!rm ./demo_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jNceymDt0Uec"
      },
      "outputs": [],
      "source": [
        "# 数据集处理\n",
        "import re\n",
        "\n",
        "from paddlenlp.datasets import load_dataset\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
        "    text = re.sub(r\"\\\\n\\n\", \".\", text)\n",
        "    return text\n",
        "\n",
        "# 定义读取数据集函数\n",
        "def read_custom_data(is_test=False, is_one_hot=True):\n",
        "\n",
        "    file_num = 6 if is_test else 48\n",
        "    # 定义数据集和测试集路径，如果需要导入自定义数据集，请修改filepath即可\n",
        "    filepath = '/home/work/data/raw_data/test/' if is_test else '/home/work/data/raw_data/train/'\n",
        "\n",
        "    for i in range(file_num):\n",
        "        f = open('{}labeled_{}.txt'.format(filepath, i))\n",
        "        while True:\n",
        "            line = f.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            data = line.strip().split('\\t')\n",
        "            # 标签用One-hot表示\n",
        "            if is_one_hot:\n",
        "                labels = [float(1) if str(i) in data[1].split(',') else float(0) for i in range(20)]\n",
        "            else:\n",
        "                labels = [int(d) for d in data[1].split(',')]\n",
        "            yield {\"text\": clean_text(data[0]), \"labels\": labels}\n",
        "        f.close()\n",
        "\n",
        "#定义label意义映射,自定义数据集这里需要修改\n",
        "label_vocab = {\n",
        "    0: \"婚后有子女\",\n",
        "    1: \"限制行为能力子女抚养\",\n",
        "    2: \"有夫妻共同财产\",\n",
        "    3: \"支付抚养费\",\n",
        "    4: \"不动产分割\",\n",
        "    5: \"婚后分居\",\n",
        "    6: \"二次起诉离婚\",\n",
        "    7: \"按月给付抚养费\",\n",
        "    8: \"准予离婚\",\n",
        "    9: \"有夫妻共同债务\",\n",
        "    10: \"婚前个人财产\",\n",
        "    11: \"法定离婚\",\n",
        "    12: \"不履行家庭义务\",\n",
        "    13: \"存在非婚生子\",\n",
        "    14: \"适当帮助\",\n",
        "    15: \"不履行离婚协议\",\n",
        "    16: \"损害赔偿\",\n",
        "    17: \"感情不和分居满二年\",\n",
        "    18: \"子女随非抚养权人生活\",\n",
        "    19: \"婚后个人财产\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VAGyxoNS0Uec"
      },
      "outputs": [],
      "source": [
        "# load_dataset()创建数据集\n",
        "train_ds = load_dataset(read_custom_data, is_test=False, lazy=False) \n",
        "test_ds = load_dataset(read_custom_data, is_test=True, lazy=False)\n",
        "\n",
        "# lazy=False，数据集返回为MapDataset类型\n",
        "print(\"数据类型:\", type(train_ds))\n",
        "\n",
        "# labels为One-hot标签\n",
        "print(\"训练集样例:\", train_ds[0])\n",
        "print(\"测试集样例:\", test_ds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCyELl4U0Ued"
      },
      "source": [
        "### 2.3 加载中文ERNIE 3.0预训练模型和分词器\n",
        "\n",
        "PaddleNLP中Auto模块（包括AutoModel, AutoTokenizer及各种下游任务类）提供了方便易用的接口，无需指定模型类别，即可调用不同网络结构的预训练模型。PaddleNLP的预训练模型可以很容易地通过from_pretrained()方法加载，[Transformer预训练模型汇总](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html#transformer)包含了40多个主流预训练模型，500多个模型权重。\n",
        "\n",
        "\n",
        "AutoModelForSequenceClassification可用于多标签分类，通过预训练模型获取输入文本的表示，之后将文本表示进行分类。PaddleNLP已经实现了ERNIE 3.0预训练模型，可以通过一行代码实现ERNIE 3.0预训练模型和分词器的加载。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3CtfrImX0Ued"
      },
      "outputs": [],
      "source": [
        "# 加载中文ERNIE 3.0预训练模型和分词器\n",
        "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"ernie-3.0-base-zh\"\n",
        "num_classes = 20\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9nDEYNT0Uee"
      },
      "source": [
        "### 2.4 基于预训练模型的数据处理\n",
        "`Dataset`中通常为原始数据，需要经过一定的数据处理并进行采样组batch。\n",
        "* 通过`Dataset`的`map`函数，使用分词器将数据集从原始文本处理成模型的输入。\n",
        "* 定义`paddle.io.BatchSampler`和`collate_fn`构建 `paddle.io.DataLoader`。\n",
        "\n",
        "实际训练中，根据显存大小调整批大小`batch_size`和文本最大长度`max_seq_length`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "X9U5yY6Z0Uee"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import numpy as np\n",
        "\n",
        "from paddle.io import DataLoader, BatchSampler\n",
        "from paddlenlp.data import DataCollatorWithPadding\n",
        "\n",
        "# 数据预处理函数，利用分词器将文本转化为整数序列\n",
        "def preprocess_function(examples, tokenizer, max_seq_length):\n",
        "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
        "    result[\"labels\"] = examples[\"labels\"]\n",
        "    return result\n",
        "\n",
        "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=128)\n",
        "train_ds = train_ds.map(trans_func)\n",
        "test_ds = test_ds.map(trans_func)\n",
        "\n",
        "# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
        "collate_fn = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
        "train_batch_sampler = BatchSampler(train_ds, batch_size=64, shuffle=True)\n",
        "test_batch_sampler = BatchSampler(test_ds, batch_size=64, shuffle=False)\n",
        "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
        "test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WznCwzp00Uee"
      },
      "source": [
        "### 2.5 数据训练和评估\n",
        "定义训练所需的优化器、损失函数、评价指标等，就可以开始进行预模型微调任务。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "m3BGYO_p0Uef"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import paddle.nn.functional as F\n",
        "\n",
        "from demo.legalText.metric import MultiLabelReport\n",
        "\n",
        "# Adam优化器、交叉熵损失函数、自定义MultiLabelReport评价指标\n",
        "optimizer = paddle.optimizer.AdamW(learning_rate=1e-4, parameters=model.parameters())\n",
        "criterion = paddle.nn.BCEWithLogitsLoss()\n",
        "metric = MultiLabelReport()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "GAExTn0v0Uef"
      },
      "outputs": [],
      "source": [
        "from demo.legalText.eval import evaluate\n",
        "epochs = 10 # 训练轮次\n",
        "ckpt_dir = \"/home/work/PretrainedModel/legalText\" #训练过程中保存模型参数的文件夹\n",
        "\n",
        "global_step = 0 #迭代次数\n",
        "tic_train = time.time()\n",
        "best_f1_score = 0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for step, batch in enumerate(train_data_loader, start=1):\n",
        "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
        "\n",
        "        # 计算模型输出、损失函数值、分类概率值、准确率、f1分数\n",
        "        logits = model(input_ids, token_type_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        probs = F.sigmoid(logits)\n",
        "        metric.update(probs, labels)\n",
        "        auc, f1_score, _, _ = metric.accumulate()\n",
        "\n",
        "        # 每迭代10次，打印损失函数值、准确率、f1分数、计算速度\n",
        "        global_step += 1\n",
        "        if global_step % 10 == 0:\n",
        "            print(\n",
        "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, auc: %.5f, f1 score: %.5f, speed: %.2f step/s\"\n",
        "                % (global_step, epoch, step, loss, auc, f1_score,\n",
        "                    10 / (time.time() - tic_train)))\n",
        "            tic_train = time.time()\n",
        "        \n",
        "        # 反向梯度回传，更新参数\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.clear_grad()\n",
        "        # 每迭代40次，评估当前训练的模型、保存当前最佳模型参数和分词器的词表等\n",
        "        if global_step % 40 == 0:\n",
        "            save_dir = ckpt_dir\n",
        "            if not os.path.exists(save_dir):\n",
        "                os.makedirs(save_dir)\n",
        "            eval_f1_score = evaluate(model, criterion, metric, test_data_loader, label_vocab, if_return_results=False)\n",
        "            if eval_f1_score > best_f1_score:\n",
        "                best_f1_score = eval_f1_score\n",
        "                model.save_pretrained(save_dir)\n",
        "                tokenizer.save_pretrained(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2v48cTt0Uef"
      },
      "source": [
        "模型训练过程中会输出如下日志:\n",
        "```\n",
        "global step 10, epoch: 1, batch: 10, loss: 0.34328, auc: 0.63276, f1 score: 0.22379, speed: 1.26 step/s\n",
        "global step 20, epoch: 1, batch: 20, loss: 0.27681, auc: 0.68451, f1 score: 0.25070, speed: 0.90 step/s\n",
        "global step 30, epoch: 1, batch: 30, loss: 0.21992, auc: 0.73419, f1 score: 0.29259, speed: 0.74 step/s\n",
        "global step 40, epoch: 1, batch: 40, loss: 0.18223, auc: 0.78311, f1 score: 0.35254, speed: 0.62 step/s\n",
        "eval loss: 0.17606, auc: 0.91074, f1 score: 0.72432, precison: 0.69068, recall: 0.76141\n",
        "[2022-05-13 16:36:18,232] [    INFO] - tokenizer config file saved in ernie_ckpt_1/tokenizer_config.json\n",
        "[2022-05-13 16:36:18,235] [    INFO] - Special tokens file saved in ernie_ckpt_1/special_tokens_map.json\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA22lrja0Ueg"
      },
      "source": [
        "### 2.6 多标签分类预测结果预测\n",
        "加载微调好的模型参数进行情感分析预测，并保存预测结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "faNPH-nk0Ueg"
      },
      "outputs": [],
      "source": [
        "from demo.legalText.eval import evaluate\n",
        "\n",
        "# 可以加载训练好的模型参数结果查看模型训练结果\n",
        "model.set_dict(paddle.load('/home/work/PretrainedModel/legalText/model_state.pdparams'))\n",
        "\n",
        "# 模型在测试集中表现\n",
        "print(\"ERNIE 3.0 在法律文本多标签分类test集表现\", end= \" \")\n",
        "results = evaluate(model, criterion, metric, test_data_loader, label_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "f0sOGr_-0Ueg"
      },
      "outputs": [],
      "source": [
        "test_ds = load_dataset(read_custom_data, is_test=True, is_one_hot=False, lazy=False)\n",
        "res_dir = \"/home/work/results/legalText\"\n",
        "if not os.path.exists(res_dir):\n",
        "    os.makedirs(res_dir)\n",
        "with open(os.path.join(res_dir, \"multi_label.tsv\"), 'w', encoding=\"utf8\") as f:\n",
        "    f.write(\"text\\tprediction\\n\")\n",
        "    for i, pred in enumerate(results):\n",
        "        f.write(test_ds[i]['text']+\"\\t\"+pred+\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ci4--wc0Ueh"
      },
      "source": [
        "法律多标签文本预测结果示例:\n",
        "![](https://ai-studio-static-online.cdn.bcebos.com/ddf3132abd7f4e3b9c29403a4624492b180d4bcc6eef4390ac962142ce276b13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s2nFvVB0Ueh"
      },
      "source": [
        "# 三.中文语义匹配实战\n",
        "\n",
        "本项目将介绍如何基于PaddleNLP利用ERNIE 3.0预训练模型微调并进行中文语义匹配预测。本项目主要包括“什么是语义匹配任务”、“ERNIE 3.0模型”、“如何使用ERNIE 3.0中文预训练模型进行语义匹配”等三个部分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPoINI1l0Ueh"
      },
      "source": [
        "## 1. 什么是语义匹配任务\n",
        "\n",
        "文本语义匹配，通俗来讲就是判断两个文本的语义是否相同。文本语义匹配是自然语言处理中最基本的任务之一，语义匹配在搜索匹配、智能客服、新闻推荐等都有广泛的应用：在搜索匹配中，利用语义匹配技术，根据查询项检索与其相似内容；在智能客服中，通过将问题与候选答案或对话与回复相匹配，提高效率节约人工成本；在新闻推荐中，为用户推荐浏览过的新闻标题相似的新闻，提供新闻个性化推荐服务。\n",
        "\n",
        "语义匹配任务常见两种训练范式:Point-wise和Pair-wise。其中单塔Point-wise匹配模型适合直接对文本对进行二分类的应用场景: 例如判断 2 个文本是否为语义相似；Pair-wise 匹配模型适合将文本对相似度作为特征之一输入到上层排序模块进行排序的应用场景。我们将示范如何使用ERNIE 3.0单塔Point-wise匹配模型在中文语义匹配数据集LCQMC进行微调。\n",
        "\n",
        "![](https://ai-studio-static-online.cdn.bcebos.com/8330b7a380b347d7a087d5add94264deeb087f32aea84d9e8ce498dec9fa0869)\n",
        "<font size=2><center>在百度搜索\"什么是语义匹配\"时会推荐相似搜索内容</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_92-sBjm0Ueh"
      },
      "source": [
        "## 2. 如何使用ERNIE 3.0中文预训练模型进行语义匹配任务\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7ukGSvG0Uei"
      },
      "source": [
        "### 2.1 环境准备"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tIKP79Uf0Uei"
      },
      "outputs": [],
      "source": [
        "# 安装pyzmq（若中文匹配中已经安装，这里可跳过）\n",
        "!pip install pyzmq==18.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLFvgDOC0Uei"
      },
      "source": [
        "### 2.2 加载中文语义匹配数据集LCQMC\n",
        "LCQMC（Large-scale Chinese Question Matching Corpus）中文语义匹配数据集, 基于百度知道相似问题推荐构造的通问句中文语义匹配数据集，目的是为了解决在中文领域大规模问题匹配数据集的缺失。该数据集从百度知道不同领域的用户问题中抽取构建数据，数据集示例：\n",
        "```\n",
        "query    title    label\n",
        "喜欢打篮球的男生喜欢什么样的女生\t爱打篮球的男生喜欢什么样的女生\t1\n",
        "我手机丢了，我想换个手机\t我想买个新手机，求推荐\t1\n",
        "大家觉得她好看吗\t大家觉得跑男好看吗\t0\n",
        "晚上睡觉带着耳机听音乐有什么害处吗？\t孕妇可以戴耳机听音乐吗?\t0\n",
        "```\n",
        "其中1表示语义相似，0表示语义不相似，PaddleNLP已经内置该数据集，一键即可加载。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "OWPbmzon0Uei"
      },
      "outputs": [],
      "source": [
        "# 加载中文语义匹配数据集lcqmc\n",
        "from paddlenlp.datasets import load_dataset\n",
        "\n",
        "train_ds, dev_ds, test_ds = load_dataset(\"lcqmc\", splits=[\"train\", \"dev\", \"test\"])\n",
        "\n",
        "# 数据集返回为MapDataset类型\n",
        "print(\"数据类型:\", type(train_ds))\n",
        "# label代表标签，测试集中不包含标签信息\n",
        "print(\"训练集样例:\", train_ds[0])\n",
        "print(\"验证集样例:\", dev_ds[0])\n",
        "print(\"测试集样例:\", test_ds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjxvn2hv0Uej"
      },
      "source": [
        "### 2.3 加载中文ERNIE 3.0预训练模型和分词器\n",
        "\n",
        "PaddleNLP中Auto模块（包括AutoModel, AutoTokenizer及各种下游任务类）提供了方便易用的接口，无需指定模型类别，即可调用不同网络结构的预训练模型。PaddleNLP的预训练模型可以很容易地通过from_pretrained()方法加载，[Transformer预训练模型汇总](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html#transformer)包含了40多个主流预训练模型，500多个模型权重。\n",
        "\n",
        "\n",
        "AutoModelForSequenceClassification可用于Point-wise方式的二分类语义匹配任务，通过预训练模型获取输入文本对（query-title）的表示，之后将文本表示进行分类。PaddleNLP已经实现了ERNIE 3.0预训练模型，可以通过一行代码实现ERNIE 3.0预训练模型和分词器的加载。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "qx1dRFEF0Uej"
      },
      "outputs": [],
      "source": [
        "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"ernie-3.0-base-zh\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=len(train_ds.label_list))\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T74TtgWg0Uej"
      },
      "source": [
        "### 2.4 基于预训练模型的数据处理\n",
        "`Dataset`中通常为原始数据，需要经过一定的数据处理并进行采样组batch。\n",
        "* 通过`Dataset`的`map`函数，使用分词器将数据集中query文本和title文本拼接，从原始文本处理成模型的输入。\n",
        "* 定义`paddle.io.BatchSampler`和`collate_fn`构建 `paddle.io.DataLoader`。\n",
        "\n",
        "实际训练中，根据显存大小调整批大小`batch_size`和文本最大长度`max_seq_length`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tJTFgyQQ0Uej"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import numpy as np\n",
        "\n",
        "from paddle.io import DataLoader, BatchSampler\n",
        "from paddlenlp.data import DataCollatorWithPadding\n",
        "\n",
        "# 数据预处理函数，利用分词器将文本转化为整数序列\n",
        "def preprocess_function(examples, tokenizer, max_seq_length, is_test=False):\n",
        "    \n",
        "    result = tokenizer(text=examples[\"query\"], text_pair=examples[\"title\"], max_seq_len=max_seq_length)\n",
        "    if not is_test:\n",
        "        result[\"labels\"] = examples[\"label\"]\n",
        "    return result\n",
        "\n",
        "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=128)\n",
        "train_ds = train_ds.map(trans_func)\n",
        "dev_ds = dev_ds.map(trans_func)\n",
        "\n",
        "# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
        "collate_fn = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
        "train_batch_sampler = BatchSampler(train_ds, batch_size=64, shuffle=True)\n",
        "dev_batch_sampler = BatchSampler(dev_ds, batch_size=128, shuffle=False)\n",
        "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
        "dev_data_loader = DataLoader(dataset=dev_ds, batch_sampler=dev_batch_sampler, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg_8DcfP0Uek"
      },
      "source": [
        "### 2.5 数据训练和评估\n",
        "定义训练所需的优化器、损失函数、评价指标等，就可以开始进行预模型微调任务。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TG-On6yI0Uek"
      },
      "outputs": [],
      "source": [
        "# Adam优化器、交叉熵损失函数、accuracy评价指标\n",
        "optimizer = paddle.optimizer.AdamW(learning_rate=5e-5, parameters=model.parameters())\n",
        "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
        "metric = paddle.metric.Accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_9_ku8JK0Uek"
      },
      "outputs": [],
      "source": [
        "# 开始训练\n",
        "import time\n",
        "import paddle.nn.functional as F\n",
        "\n",
        "from demo.semanticMatchCn.eval import evaluate\n",
        "\n",
        "epochs = 1 # 训练轮次\n",
        "ckpt_dir = \"/home/work/PretrainedModel/semanticMatchCn\" #训练过程中保存模型参数的文件夹\n",
        "best_acc = 0\n",
        "best_step = 0\n",
        "global_step = 0 #迭代次数\n",
        "tic_train = time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for step, batch in enumerate(train_data_loader, start=1):\n",
        "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
        "\n",
        "        # 计算模型输出、损失函数值、分类概率值、准确率\n",
        "        logits = model(input_ids, token_type_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        probs = F.softmax(logits, axis=1)\n",
        "        correct = metric.compute(probs, labels)\n",
        "        metric.update(correct)\n",
        "        acc = metric.accumulate()\n",
        "\n",
        "        # 每迭代10次，打印损失函数值、准确率、计算速度\n",
        "        global_step += 1\n",
        "        if global_step % 10 == 0:\n",
        "            print(\n",
        "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
        "                % (global_step, epoch, step, loss, acc,\n",
        "                    10 / (time.time() - tic_train)))\n",
        "            tic_train = time.time()\n",
        "        \n",
        "        # 反向梯度回传，更新参数\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.clear_grad()\n",
        "\n",
        "        # 每迭代100次，评估当前训练的模型、保存当前最佳模型参数和分词器的词表等\n",
        "        if global_step % 100 == 0:\n",
        "            save_dir = ckpt_dir\n",
        "            if not os.path.exists(save_dir):\n",
        "                os.makedirs(save_dir)\n",
        "            print(\"global step\", global_step, end=' ')\n",
        "            acc_eval = evaluate(model, criterion, metric, dev_data_loader)\n",
        "            if acc_eval > best_acc:\n",
        "                best_acc = acc_eval\n",
        "                best_step = global_step\n",
        "\n",
        "                model.save_pretrained(save_dir)\n",
        "                tokenizer.save_pretrained(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr7Tp3810Uel"
      },
      "source": [
        "模型训练过程中会输出如下日志:\n",
        "```\n",
        "global step 10, epoch: 1, batch: 10, loss: 0.55404, accu: 0.64844, speed: 5.08 step/s\n",
        "global step 20, epoch: 1, batch: 20, loss: 0.29190, accu: 0.73125, speed: 5.36 step/s\n",
        "global step 30, epoch: 1, batch: 30, loss: 0.37968, accu: 0.76719, speed: 5.51 step/s\n",
        "global step 40, epoch: 1, batch: 40, loss: 0.34762, accu: 0.79453, speed: 5.26 step/s\n",
        "global step 50, epoch: 1, batch: 50, loss: 0.29682, accu: 0.80969, speed: 5.03 step/s\n",
        "global step 60, epoch: 1, batch: 60, loss: 0.24128, accu: 0.82344, speed: 5.07 step/s\n",
        "global step 70, epoch: 1, batch: 70, loss: 0.24660, accu: 0.83058, speed: 5.35 step/s\n",
        "global step 80, epoch: 1, batch: 80, loss: 0.32774, accu: 0.83828, speed: 5.46 step/s\n",
        "global step 90, epoch: 1, batch: 90, loss: 0.40908, accu: 0.84306, speed: 5.55 step/s\n",
        "global step 100, epoch: 1, batch: 100, loss: 0.38926, accu: 0.84750, speed: 5.18 step/s\n",
        "global step 100 eval dev loss: 0.35993, accu: 0.84276\n",
        "[2022-05-13 16:54:52,751] [    INFO] - tokenizer config file saved in ernie_ckpt_1/tokenizer_config.json\n",
        "[2022-05-13 16:54:52,754] [    INFO] - Special tokens file saved in ernie_ckpt_1/special_tokens_map.json\n",
        "...\n",
        "```\n",
        "在单卡运行环境中，一个epoch大约需要22分钟"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JO2CbSQg0Uem"
      },
      "outputs": [],
      "source": [
        "from demo.semanticMatchCn.eval import evaluate\n",
        "\n",
        "# 加载训练好的模型参数结果查看模型训练结果\n",
        "params_path = '/home/work/PretrainedModel/semanticMatchCn/model_state.pdparams'\n",
        "state_dict = paddle.load(params_path)\n",
        "model.set_dict(state_dict)\n",
        "print('ERNIE 3.0 在lcqmc的dev集表现', end=' ')\n",
        "eval_acc = evaluate(model, criterion, metric, dev_data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKbXOlAY0Uem"
      },
      "source": [
        "### 2.6 语义匹配结果预测与保存\n",
        "加载微调好的模型参数进行语义匹配预测，并保存预测结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "R7HN-m4x0Uem"
      },
      "outputs": [],
      "source": [
        "# 测试集数据预处理，利用分词器将文本转化为整数序列\n",
        "trans_func_test = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=128, is_test=True)\n",
        "test_ds_trans = test_ds.map(trans_func_test)\n",
        "\n",
        "# 进行采样组batch\n",
        "collate_fn_test = DataCollatorWithPadding(tokenizer)\n",
        "test_batch_sampler = BatchSampler(test_ds_trans, batch_size=32, shuffle=False)\n",
        "test_data_loader = DataLoader(dataset=test_ds_trans, batch_sampler=test_batch_sampler, collate_fn=collate_fn_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1OvrrWDt0Uen"
      },
      "outputs": [],
      "source": [
        "# 模型预测分类结果\n",
        "import paddle.nn.functional as F\n",
        "\n",
        "label_map = {0: '不相似', 1: '相似'}\n",
        "results = []\n",
        "model.eval()\n",
        "for batch in test_data_loader:\n",
        "    input_ids, token_type_ids = batch['input_ids'], batch['token_type_ids']\n",
        "    logits = model(batch['input_ids'], batch['token_type_ids'])\n",
        "    probs = F.softmax(logits, axis=-1)\n",
        "    idx = paddle.argmax(probs, axis=1).numpy()\n",
        "    idx = idx.tolist()\n",
        "    preds = [label_map[i] for i in idx]\n",
        "    results.extend(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "L-DBoF710Uen"
      },
      "outputs": [],
      "source": [
        "# 存储LCQMC预测结果\n",
        "test_ds = load_dataset(\"lcqmc\", splits=[\"test\"])    \n",
        "res_dir = \"/home/work/results/semanticMatchCn\"\n",
        "if not os.path.exists(res_dir):\n",
        "    os.makedirs(res_dir)\n",
        "with open(os.path.join(res_dir, \"lcqmc.tsv\"), 'w', encoding=\"utf8\") as f:\n",
        "    f.write(\"label\\tquery\\ttitle\\n\")\n",
        "    for i, pred in enumerate(results):\n",
        "        f.write(pred+\"\\t\"+test_ds[i]['query']+\"\\t\"+test_ds[i]['title']+\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC37KGbw0Uen"
      },
      "source": [
        "LCQMC预测存储结果:\n",
        "\n",
        "![](https://ai-studio-static-online.cdn.bcebos.com/210b21c8d9154320839bbd47541be9c80dd1dfd381e24d00b52c4da5799114cf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-iKtlBW0Uen"
      },
      "source": [
        "# 四.MSRA序列标注实战\n",
        "\n",
        "本项目将介绍如何基于PaddleNLP利用ERNIE 3.0预训练模型微调并进行中文序列标注预测。本项目主要包括“什么是序列标注”、“ERNIE 3.0模型”、“如何使用ERNIE 3.0中文预训练模型进行MSRA序列标注”等三个部分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqkNKGgT0Ueo"
      },
      "source": [
        "## 1. 什么是序列标注\n",
        "\n",
        "序列标注(Sequence Tagging)是经典的自然语言处理问题，可以用于解决一系列字符分类问题，例如分词、词性标注(POS tagging)、命名实体识别(Named Entity Recognition，NER)、关键词抽取、语义角色标注(Semantic Role Labeling)、槽位抽取(Slot Filling)。在现实场景中，序列标注技术可以帮助完成简历、快递单、病例医疗实体信息抽取等。\n",
        "\n",
        "在序列标注任务中，一般会定义一个标签集合来表示所有预测结果。对于输入的序列，任务目标是对序列中所有字符进行标记。在深度学习中，通常将序列标注问题视为分类问题，对输入序列的每一个token进行一次多分类任务进行训练预测。\n",
        "\n",
        "![](https://ai-studio-static-online.cdn.bcebos.com/7f931b66e5fc4e4eaf8d1dc60a950413c16b7188380a480c9c5dba2ec768a4fb)\n",
        "\n",
        "\n",
        "<font size=2><center>对输入序列进行标记</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF_UeVPN0Ueo"
      },
      "source": [
        "## 2. 如何使用ERNIE 3.0中文预训练模型进行MSRA序列标注\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx1xBozz0Ueo"
      },
      "source": [
        "### 2.1 环境准备"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jXnSkMc40Ueo"
      },
      "outputs": [],
      "source": [
        "# 若“法律文本多标签”或“中文语义匹配”项目已安装，则可跳过此步骤\n",
        "!pip install pyzmq==18.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75sd0hE80Uep"
      },
      "source": [
        "### 2.2 加载MSRA-NER数据集\n",
        "\n",
        "MSRA-NER 数据集由微软亚研院发布，其目标是识别文本中具有特定意义的实体，主要包括人名、地名、机构名等。PaddleNLP已经内置该数据集，一键即可加载。PaddleNLP集成的数据集MSRA-NER数据集对文件格式做了调整：每一行文本、标签以特殊字符\"\\t\"进行分隔，每个字之间以特殊字符\"\\002\"分隔。示例如下：\n",
        "```\n",
        "不\\002久\\002前\\002，\\002中\\002国\\002共\\002产\\002党\\002召\\002开\\002了\\002举\\002世\\002瞩\\002目\\002的\\002第\\002十\\002五\\002次\\002全\\002国\\002代\\002表\\002大\\002会\\002。O\\002O\\002O\\002O\\002B-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002B-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002I-ORG\\002O\n",
        "\n",
        "这\\002次\\002代\\002表\\002大\\002会\\002是\\002在\\002中\\002国\\002改\\002革\\002开\\002放\\002和\\002社\\002会\\002主\\002义\\002现\\002代\\002化\\002建\\002设\\002发\\002展\\002的\\002关\\002键\\002时\\002刻\\002召\\002开\\002的\\002历\\002史\\002性\\002会\\002议\\002。O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002B-LOC\\002I-LOC\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\\002O\n",
        "```\n",
        "\n",
        "加载MSRA_NER数据集为BIO标注集：\n",
        "* B-PER、I-PER代表人名首字、人名非首字。\n",
        "* B-LOC、I-LOC代表地名首字、地名非首字。\n",
        "* B-ORG、I-ORG代表组织机构名首字、组织机构名非首字。\n",
        "* O代表该字不属于命名实体的一部分。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "M7ZpQbZ30Uep"
      },
      "outputs": [],
      "source": [
        "# 加载MSRA_NER数据集\n",
        "from paddlenlp.datasets import load_dataset\n",
        "\n",
        "train_ds, test_ds = load_dataset('msra_ner', splits=('train', 'test'), lazy=False)\n",
        "label_vocab = {label:label_id for label_id, label in enumerate(train_ds.label_list)}\n",
        "\n",
        "# 数据集返回类型为MapDataset\n",
        "print(\"数据类型:\", type(train_ds))\n",
        "print(\"数据标签:\", label_vocab)\n",
        "\n",
        "# 每条数据包含一句文本和这个文本中每个汉字以及数字对应的label标签\n",
        "print(\"训练集样例:\", train_ds[0])\n",
        "print(\"测试集样例:\", test_ds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROPiFZ1T0Uep"
      },
      "source": [
        "### 2.3 加载中文ERNIE 3.0预训练模型和分词器\n",
        "\n",
        "PaddleNLP中[Auto模块](https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.transformers.auto.modeling.html)（包括AutoModel, AutoTokenizer及各种下游任务类）提供了方便易用的接口，无需指定模型类别，即可调用不同网络结构的预训练模型。PaddleNLP的预训练模型可以很容易地通过from_pretrained()方法加载，[Transformer预训练模型汇总](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html#transformer)包含了40多个主流预训练模型，500多个模型权重。\n",
        "\n",
        "\n",
        "`AutoForTokenClassification`可用于序列标注，通过预训练模型获取输入文本每个token的表示，之后将token表示进行分类。PaddleNLP已经实现了ERNIE 3.0预训练模型，可以通过一行代码实现ERNIE 3.0预训练模型和分词器的加载。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "J5onlleF0Ueq"
      },
      "outputs": [],
      "source": [
        "from paddlenlp.transformers import AutoModelForTokenClassification\n",
        "from paddlenlp.transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"ernie-3.0-base-zh\"\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_classes=len(train_ds.label_list))\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBZvq8kw0Ueq"
      },
      "source": [
        "### 2.4 基于预训练模型的数据处理\n",
        "`Dataset`中通常为原始数据，需要经过一定的数据处理转成可输入模型的数据并进行采样组batch。\n",
        "* 通过`Dataset`的`map`函数，使用分词器将数据集从原始文本处理成模型的输入。\n",
        "* 定义`paddle.io.BatchSampler`和`collate_fn`构建 `paddle.io.DataLoader`。\n",
        "\n",
        "实际训练中，根据显存大小调整批大小`batch_size`和文本最大长度`max_seq_length`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "xZAuiiqB0Ueq"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import numpy as np\n",
        "\n",
        "from paddle.io import DataLoader, BatchSampler\n",
        "from paddlenlp.data import DataCollatorForTokenClassification\n",
        "\n",
        "# 数据预处理函数，利用分词器将文本转化为整数序列\n",
        "def preprocess_function(example, tokenizer, label_vocab, max_seq_length=128):\n",
        "\n",
        "    labels = example['labels']\n",
        "    tokens = example['tokens']\n",
        "    no_entity_id = label_vocab['O']\n",
        "\n",
        "    tokenized_input = tokenizer(tokens, return_length=True, is_split_into_words=True, max_seq_len=max_seq_length)\n",
        "\n",
        "    # 保证label与input_ids长度一致\n",
        "    # -2 for [CLS] and [SEP]\n",
        "    if len(tokenized_input['input_ids']) - 2 < len(labels):\n",
        "        labels = labels[:len(tokenized_input['input_ids']) - 2]\n",
        "    tokenized_input['labels'] = [no_entity_id] + labels + [no_entity_id]\n",
        "    tokenized_input['labels'] += [no_entity_id] * (len(tokenized_input['input_ids']) - len(tokenized_input['labels'])) \n",
        "    \n",
        "    return tokenized_input\n",
        "\n",
        "\n",
        "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, label_vocab=label_vocab, max_seq_length=128)\n",
        "train_ds = train_ds.map(trans_func)\n",
        "test_ds = test_ds.map(trans_func)\n",
        "\n",
        "# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
        "collate_fn = DataCollatorForTokenClassification(tokenizer=tokenizer, label_pad_token_id=-1)\n",
        "\n",
        "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
        "train_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\n",
        "test_batch_sampler = BatchSampler(test_ds, batch_size=32, shuffle=False)\n",
        "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
        "test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5d0Tnjd0Ueq"
      },
      "source": [
        "### 2.5 数据训练和评估\n",
        "定义训练所需的优化器、损失函数、评价指标等，就可以开始进行预模型微调任务。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dEtQYWz80Uer"
      },
      "outputs": [],
      "source": [
        "from paddlenlp.metrics import ChunkEvaluator\n",
        "\n",
        "# Adam优化器、交叉熵损失函数、ChunkEvaluator评价指标\n",
        "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())\n",
        "criterion = paddle.nn.loss.CrossEntropyLoss(ignore_index=-1)\n",
        "metric = ChunkEvaluator(label_list=train_ds.label_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2KPyyFKC0Uer"
      },
      "outputs": [],
      "source": [
        "# 开始训练\n",
        "import time\n",
        "import paddle.nn.functional as F\n",
        "\n",
        "from demo.sequenceTagging.utils import evaluate\n",
        "\n",
        "epochs = 10 # 训练轮次\n",
        "ckpt_dir = \"/home/work/PretrainedModel/sequenceTagging\" #训练过程中保存模型参数的文件夹\n",
        "best_f1_score = 0\n",
        "best_step = 0\n",
        "global_step = 0 #迭代次数\n",
        "tic_train = time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for step, batch in enumerate(train_data_loader, start=1):\n",
        "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
        "\n",
        "        # 计算模型输出、损失函数值\n",
        "        logits = model(input_ids, token_type_ids)\n",
        "        loss =  paddle.mean(criterion(logits, labels))\n",
        "\n",
        "        # 每迭代10次，打印损失函数值、计算速度\n",
        "        global_step += 1\n",
        "        if global_step % 10 == 0:\n",
        "            print(\n",
        "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, speed: %.2f step/s\"\n",
        "                % (global_step, epoch, step, loss, 10 / (time.time() - tic_train)))\n",
        "            tic_train = time.time()\n",
        "        \n",
        "        # 反向梯度回传\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.clear_grad()\n",
        "\n",
        "        # 每迭代200次，评估当前训练的模型、保存当前最佳模型参数和分词器的词表等\n",
        "        if global_step % 200 == 0:\n",
        "            save_dir = ckpt_dir\n",
        "            if not os.path.exists(save_dir):\n",
        "                os.makedirs(save_dir)\n",
        "            print('global_step', global_step, end=' ')\n",
        "            f1_score_eval = evaluate(model, metric, test_data_loader)\n",
        "            if f1_score_eval > best_f1_score:\n",
        "                best_f1_score = f1_score_eval\n",
        "                best_step = global_step\n",
        "\n",
        "                model.save_pretrained(save_dir)\n",
        "                tokenizer.save_pretrained(save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaoV_jRR0Uer"
      },
      "source": [
        "模型每训练迭代100次会评估一次，训练过程中会输出如下日志:\n",
        "```\n",
        "global step 10, epoch: 1, batch: 10, loss: 0.69060, speed: 5.02 step/s\n",
        "global step 20, epoch: 1, batch: 20, loss: 0.57494, speed: 5.07 step/s\n",
        "global step 30, epoch: 1, batch: 30, loss: 0.55177, speed: 5.18 step/s\n",
        "global step 40, epoch: 1, batch: 40, loss: 0.44063, speed: 4.91 step/s\n",
        "global step 50, epoch: 1, batch: 50, loss: 0.19791, speed: 5.00 step/s\n",
        "global step 60, epoch: 1, batch: 60, loss: 0.28906, speed: 5.02 step/s\n",
        "global step 70, epoch: 1, batch: 70, loss: 0.17547, speed: 5.18 step/s\n",
        "global step 80, epoch: 1, batch: 80, loss: 0.17136, speed: 4.98 step/s\n",
        "global step 90, epoch: 1, batch: 90, loss: 0.20389, speed: 4.90 step/s\n",
        "global step 100, epoch: 1, batch: 100, loss: 0.14262, speed: 4.89 step/s\n",
        "global step 110, epoch: 1, batch: 110, loss: 0.09750, speed: 5.26 step/s\n",
        "global step 120, epoch: 1, batch: 120, loss: 0.16538, speed: 5.09 step/s\n",
        "global step 130, epoch: 1, batch: 130, loss: 0.10952, speed: 4.93 step/s\n",
        "global step 140, epoch: 1, batch: 140, loss: 0.16549, speed: 5.21 step/s\n",
        "global step 150, epoch: 1, batch: 150, loss: 0.14116, speed: 5.15 step/s\n",
        "global step 160, epoch: 1, batch: 160, loss: 0.10677, speed: 4.90 step/s\n",
        "global step 170, epoch: 1, batch: 170, loss: 0.05065, speed: 5.30 step/s\n",
        "global step 180, epoch: 1, batch: 180, loss: 0.14484, speed: 4.99 step/s\n",
        "global step 190, epoch: 1, batch: 190, loss: 0.07996, speed: 4.92 step/s\n",
        "global step 200, epoch: 1, batch: 200, loss: 0.10816, speed: 5.11 step/s\n",
        "global_step 200 eval precision: 0.759136 - recall: 0.855165 - f1: 0.804294\n",
        "[2022-05-13 16:05:59,091] [    INFO] - tokenizer config file saved in ernie_ckpt_1/tokenizer_config.json\n",
        "[2022-05-13 16:05:59,094] [    INFO] - Special tokens file saved in ernie_ckpt_1/special_tokens_map.json\n",
        "...\n",
        "```\n",
        "10个epoch预计训练时间60分钟。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lRXjvxr0Ues"
      },
      "source": [
        "### 2.6 序列标注结果预测与保存\n",
        "加载微调好的模型参数进行情感分析预测，并保存预测结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TgbS5czc0Ues"
      },
      "outputs": [],
      "source": [
        "# 测试集结果评估\n",
        "from demo.sequenceTagging.utils import parse_decodes\n",
        "\n",
        "# 加载训练好的模型参数结果查看模型训练结果\n",
        "# 加载最佳模型参数\n",
        "model.set_dict(paddle.load('/home/work/PretrainedModel/sequenceTagging/model_state.pdparams'))\n",
        "model.eval()\n",
        "metric.reset()\n",
        "\n",
        "pred_list = []\n",
        "len_list = []\n",
        "\n",
        "for step, batch in enumerate(test_data_loader, start=1):\n",
        "    input_ids, token_type_ids, labels, lens = batch['input_ids'], batch['token_type_ids'], batch['labels'], batch['seq_len']\n",
        "\n",
        "    logits = model(input_ids, token_type_ids)\n",
        "    preds = paddle.argmax(logits, axis=-1)\n",
        "\n",
        "    n_infer, n_label, n_correct = metric.compute(lens, preds, labels)\n",
        "    metric.update(n_infer.numpy(), n_label.numpy(), n_correct.numpy())\n",
        "\n",
        "    pred_list.append(preds.numpy())\n",
        "    len_list.append(lens.numpy())\n",
        "\n",
        "precision, recall, f1_score = metric.accumulate()\n",
        "print(\"ERNIE 3.0 在msra_ner的test集表现 -precision: %f - recall: %f - f1: %f\" %(precision, recall, f1_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDTUUADz0Ues"
      },
      "source": [
        "我们根据模型预测结果对文本进行后处理，对文本序列进行标注，具体的标签含义如下：\n",
        "* ‘O’: no special entity(其他不属于任何实体的字符,包括标点等)\n",
        "* ‘PER’: person(人名)\n",
        "* ‘ORG’: organization(组织机构)\n",
        "* ‘LOC’: location（地点）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "olLSgIKA0Ues"
      },
      "outputs": [],
      "source": [
        "# 根据预测结果对文本进行后处理\n",
        "test_ds = load_dataset('msra_ner', splits=('test')) \n",
        "preds = parse_decodes(test_ds, pred_list, len_list, label_vocab)\n",
        "\n",
        "# 查看预测结果\n",
        "print(\"查看结果\")\n",
        "print(\"\\n\".join(preds[:2]))\n",
        "\n",
        "# 保存预测结果\n",
        "res_dir = \"/home/work/results/sequenceTagging\"\n",
        "if not os.path.exists(res_dir):\n",
        "    os.makedirs(res_dir)\n",
        "with open(os.path.join(res_dir, \"results.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxDDzDQx0Uet"
      },
      "source": [
        "## 温馨提示\n",
        "每个项目训练完成生成模型文件后，若想保存当前模型版本，需要将生成的模型文件移动到/home/work/PretrainedModel目录下。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "naas"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "ernie.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}